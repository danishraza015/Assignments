{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1 What is a parameter?"
      ],
      "metadata": {
        "id": "aZPJCnkIffE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans A parameter is a value or a variable used to influence the behavior or output of a system, function, or process. The specific meaning of \"parameter\" depends on the context in which it's used. Here are some common usages:\n",
        "\n",
        "1. In Mathematics:\n",
        "A parameter is a constant in an equation that defines a family of curves or functions. For example:\n",
        "ùë¶\n",
        "=\n",
        "ùëö\n",
        "ùë•\n",
        "+\n",
        "ùëê\n",
        "y=mx+c\n",
        "Here,\n",
        "ùëö\n",
        "m (slope) and\n",
        "ùëê\n",
        "c (intercept) are parameters.\n",
        "2. In Programming:\n",
        "A parameter is a variable passed to a function or method to influence its behavior. For example, in Python:"
      ],
      "metadata": {
        "id": "6AdRI3APfyuf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkxuXZVMfeEK"
      },
      "outputs": [],
      "source": [
        "def greet(name):\n",
        "    print(f\"Hello, {name}!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "name is a parameter of the greet function.\n",
        "3. In Statistics:\n",
        "A parameter is a numerical characteristic of a population (e.g., mean or standard deviation). It's often estimated using sample data.\n",
        "4. In Engineering:\n",
        "Parameters define key characteristics that control the operation of a system or model, such as temperature, pressure, or voltage.\n",
        "5. In General Use:\n",
        "A parameter is a limit, boundary, or condition that defines or constrains a process or activity.\n",
        "In all cases, parameters serve to configure or specify aspects of a system or process."
      ],
      "metadata": {
        "id": "fxHWyCtGf6nI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2 What is correlation?\n",
        "   What does negative correlation mean?"
      ],
      "metadata": {
        "id": "JGTsEWTbgAdI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans\n",
        "Correlation refers to a statistical measure that describes the degree to which two variables move in relation to each other. It helps identify whether a relationship exists between two datasets and, if so, the direction and strength of that relationship.\n",
        "\n",
        "Types of Correlation:\n",
        "Positive Correlation:\n",
        "When one variable increases, the other tends to increase as well.\n",
        "Example: The number of hours studied and test scores might have a positive correlation.\n",
        "\n",
        "Negative Correlation:\n",
        "When one variable increases, the other tends to decrease.\n",
        "Example: The number of hours watching TV and test scores might have a negative correlation.\n",
        "\n",
        "No Correlation:\n",
        "No discernible relationship between the two variables.\n",
        "Example: The number of cats owned and test scores might have no correlation.\n",
        "\n",
        "Measuring Correlation:\n",
        "The correlation coefficient (often denoted as r) quantifies the strength and direction of a correlation. It ranges from -1 to +1:\n",
        "\n",
        "+1: Perfect positive correlation\n",
        "-1: Perfect negative correlation\n",
        "0: No correlation\n",
        "Common Types of Correlation Coefficients:\n",
        "Pearson's Correlation Coefficient: Measures the linear relationship between two continuous variables.\n",
        "Spearman's Rank Correlation: Measures the relationship between ranked (ordinal) data.\n",
        "Example:\n",
        "Given two datasets:\n",
        "\n",
        "X: 1, 2, 3, 4, 5\n",
        "Y: 2, 4, 6, 8, 10\n",
        "These datasets have a perfect positive correlation because as X increases, Y also increases proportionally.\n",
        "\n",
        "Key Considerations:\n",
        "Correlation ‚â† Causation: Just because two variables are correlated doesn't mean one causes the other.\n",
        "Spurious Correlations: Sometimes two variables are correlated purely by coincidence or due to a hidden third factor.\n",
        "A negative correlation (also known as an inverse correlation) refers to a relationship between two variables where one variable increases as the other decreases. In simpler terms, when one goes up, the other goes down.\n",
        "\n",
        "Characteristics of Negative Correlation:\n",
        "The correlation coefficient (r) for a negative correlation falls between -1 and 0.\n",
        "r = -1 indicates a perfect negative correlation, meaning the two variables move in exact opposite directions.\n",
        "r = 0 means there is no correlation.\n",
        "Examples of Negative Correlation:\n",
        "Exercise vs. Body Weight:\n",
        "The more someone exercises, the lower their body weight tends to be.\n",
        "\n",
        "Speed of a Car vs. Travel Time:\n",
        "The faster you drive, the less time it takes to reach your destination.\n",
        "\n",
        "Price of a Product vs. Demand:\n",
        "As the price of a product rises, consumer demand for it typically decreases.\n",
        "\n",
        "Graphical Representation:\n",
        "When plotted on a scatter plot, a negative correlation shows data points sloping downward from left to right.\n",
        "\n",
        "Key Point:\n",
        "While negative correlation shows an inverse relationship, it does not imply causation. Just because two variables are negatively correlated does not mean one directly causes changes in the other."
      ],
      "metadata": {
        "id": "CMqn3RpSgBqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3 Define Machine Learning. What are the main components in Machine Learning?"
      ],
      "metadata": {
        "id": "2unEL-lDgvP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans Definition of Machine Learning:\n",
        "Machine Learning (ML) is a subset of Artificial Intelligence (AI) that focuses on building systems that can learn from data, identify patterns, and make decisions with minimal human intervention. Instead of being explicitly programmed, these systems improve their performance through experience and exposure to data.\n",
        "\n",
        "Arthur Samuel, a pioneer in AI, defined it as:\n",
        "\"Machine learning is the field of study that gives computers the ability to learn without being explicitly programmed.\"\n",
        "\n",
        "Main Components in Machine Learning:\n",
        "Data\n",
        "Definition: The foundation of any ML model. This could be structured data (e.g., spreadsheets) or unstructured data (e.g., images, text).\n",
        "Types:\n",
        "Training Data: Used to train the model.\n",
        "Validation Data: Used to tune the model's parameters.\n",
        "Test Data: Used to evaluate the model's performance.\n",
        "Features\n",
        "Definition: Specific measurable properties or attributes of the data that the model uses to learn.\n",
        "Example: In predicting house prices, features could include square footage, location, and the number of bedrooms.\n",
        "Model\n",
        "Definition: The mathematical or computational structure that learns patterns from the data and makes predictions or decisions.\n",
        "Types of Models:\n",
        "Linear Regression: For predicting continuous values.\n",
        "Decision Trees: For classification tasks.\n",
        "Neural Networks: For complex patterns, like image recognition.\n",
        "Algorithm\n",
        "Definition: The method or procedure used to train the model by adjusting it based on the data.\n",
        "Examples: Gradient Descent, K-Means, Support Vector Machines (SVMs).\n",
        "Training Process\n",
        "Definition: The process where the model learns from the data by minimizing errors.\n",
        "Key Concepts:\n",
        "Loss Function: A measure of how well the model's predictions match the actual data.\n",
        "Optimization: Adjusting model parameters to minimize the loss function (e.g., via Gradient Descent).\n",
        "Evaluation Metrics\n",
        "Definition: Methods to measure how well the model performs on new, unseen data.\n",
        "Examples:\n",
        "Accuracy: For classification tasks.\n",
        "Mean Squared Error (MSE): For regression tasks.\n",
        "Precision, Recall, F1-Score: For more nuanced classification evaluations.\n",
        "Hyperparameters\n",
        "Definition: Configurable settings of the model or training process set before training begins.\n",
        "Examples: Learning rate, number of layers in a neural network, number of clusters in K-Means.\n",
        "Prediction / Inference\n",
        "Definition: Once trained, the model uses new data to make predictions or decisions.\n",
        "Feedback Loop\n",
        "Definition: Some systems continuously learn and improve by receiving new data over time, adapting their models accordingly.\n",
        "Summary of Machine Learning Process:\n",
        "Collect Data ‚ûî 2. Prepare Data ‚ûî 3. Choose a Model/Algorithm ‚ûî 4. Train the Model ‚ûî 5. Evaluate Performance ‚ûî 6. Tune Hyperparameters ‚ûî 7. Make Predictions"
      ],
      "metadata": {
        "id": "CXU-rxuFhBGl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4 How does loss value help in determining whether the model is good or not?"
      ],
      "metadata": {
        "id": "apA6h4fbhC_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans What is Loss Value?\n",
        "The loss value (or loss function) measures the difference between the predicted output of a machine learning model and the actual target values (ground truth). It quantifies how well (or poorly) the model is performing during training.\n",
        "\n",
        "A lower loss value indicates that the model's predictions are closer to the actual values, while a higher loss value means the model is making larger errors.\n",
        "\n",
        "Why is Loss Value Important?\n",
        "Model Performance Feedback:\n",
        "The loss value provides immediate feedback on how well your model is learning during training. It helps answer the question:\n",
        "\n",
        "‚ÄúHow far off are the model‚Äôs predictions from the actual targets?‚Äù\n",
        "Optimization Objective:\n",
        "The goal of training a machine learning model is to minimize the loss. Optimization algorithms like Gradient Descent adjust the model's parameters to reduce the loss value iteratively.\n",
        "\n",
        "Choosing the Best Model:\n",
        "Comparing loss values between different models helps in selecting the one that performs best on your training and validation data.\n",
        "\n",
        "Types of Loss Functions:\n",
        "For Regression Tasks:\n",
        "\n",
        "Mean Squared Error (MSE):\n",
        "MSE\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "(\n",
        "ùë¶\n",
        "ùëñ\n",
        "‚àí\n",
        "ùë¶\n",
        "^\n",
        "ùëñ\n",
        ")\n",
        "2\n",
        "MSE=\n",
        "n\n",
        "1\n",
        "‚Äã\n",
        "  \n",
        "i=1\n",
        "‚àë\n",
        "n\n",
        "‚Äã\n",
        " (y\n",
        "i\n",
        "‚Äã\n",
        " ‚àí\n",
        "y\n",
        "^\n",
        "‚Äã\n",
        "  \n",
        "i\n",
        "‚Äã\n",
        " )\n",
        "2\n",
        "\n",
        "Where\n",
        "ùë¶\n",
        "ùëñ\n",
        "y\n",
        "i\n",
        "‚Äã\n",
        "  is the actual value and\n",
        "ùë¶\n",
        "^\n",
        "ùëñ\n",
        "y\n",
        "^\n",
        "‚Äã\n",
        "  \n",
        "i\n",
        "‚Äã\n",
        "  is the predicted value.\n",
        "A lower MSE indicates better model performance.\n",
        "For Classification Tasks:\n",
        "\n",
        "Cross-Entropy Loss (Log Loss):\n",
        "Used for binary or multi-class classification. It penalizes wrong predictions with higher penalties.\n",
        "Cross-Entropy¬†Loss\n",
        "=\n",
        "‚àí\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "ùë¶\n",
        "ùëñ\n",
        "log\n",
        "‚Å°\n",
        "(\n",
        "ùë¶\n",
        "^\n",
        "ùëñ\n",
        ")\n",
        "Cross-Entropy¬†Loss=‚àí\n",
        "i=1\n",
        "‚àë\n",
        "n\n",
        "‚Äã\n",
        " y\n",
        "i\n",
        "‚Äã\n",
        " log(\n",
        "y\n",
        "^\n",
        "‚Äã\n",
        "  \n",
        "i\n",
        "‚Äã\n",
        " )\n",
        "A smaller value indicates more accurate predictions.\n",
        "Interpreting Loss Values:\n",
        "Low Loss Value:\n",
        "Indicates that the model's predictions are close to the actual targets ‚Äî the model is likely performing well.\n",
        "\n",
        "High Loss Value:\n",
        "Implies that the model's predictions are far from the actual targets ‚Äî suggesting the model is performing poorly.\n",
        "\n",
        "Decreasing Loss During Training:\n",
        "\n",
        "If the loss decreases steadily, the model is learning effectively.\n",
        "If the loss stagnates or increases, the model might be overfitting (learning the training data too closely) or underfitting (failing to capture patterns).\n",
        "Loss vs. Metrics:\n",
        "Loss:\n",
        "Used during the training process to optimize the model.\n",
        "Metrics (e.g., Accuracy, Precision, Recall):\n",
        "Used to evaluate the final model's performance on test data.\n",
        "Example:\n",
        "\n",
        "A classification model might minimize cross-entropy loss during training.\n",
        "After training, it might be evaluated using accuracy or F1-score to understand real-world performance.\n",
        "Key Takeaways:\n",
        "Loss value helps gauge how well a model is learning by measuring the discrepancy between predicted and actual outcomes.\n",
        "Minimizing the loss function is the goal of the training process.\n",
        "If the loss value is low on both training and validation data, the model is likely to perform well on unseen data."
      ],
      "metadata": {
        "id": "SNx41a0xhU38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5 What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "EGXv_Y-1hg2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans Continuous and Categorical Variables\n",
        "In statistics and machine learning, data can generally be divided into two primary types: continuous variables and categorical variables. Understanding these distinctions is crucial for data analysis and selecting appropriate models.\n",
        "\n",
        "1. Continuous Variables\n",
        "A continuous variable is a variable that can take an infinite number of possible values within a given range. These values can be measured and can include decimals or fractions.\n",
        "\n",
        "Characteristics:\n",
        "Can be numerical and measured.\n",
        "Often involves quantities like height, weight, time, and temperature.\n",
        "Can have any value within a range (e.g., 2.15, 2.16, 2.17, etc.).\n",
        "Suitable for mathematical operations (e.g., addition, subtraction, averaging).\n",
        "Examples:\n",
        "Height: 165.4 cm, 170.0 cm\n",
        "Weight: 72.5 kg, 80.1 kg\n",
        "Temperature: 37.1¬∞C, 98.6¬∞F\n",
        "Time: 2.35 hours, 45.6 minutes\n",
        "2. Categorical Variables\n",
        "A categorical variable represents data that can be divided into distinct groups or categories. These values can be counted but not measured, and they are usually labels or names rather than numbers.\n",
        "\n",
        "Characteristics:\n",
        "Can be nominal or ordinal:\n",
        "Nominal Variables: Categories without a natural order (e.g., gender, colors, car brands).\n",
        "Ordinal Variables: Categories with a meaningful order (e.g., low, medium, high; grades like A, B, C).\n",
        "Cannot perform meaningful mathematical operations on the categories.\n",
        "Examples:\n",
        "Gender: Male, Female, Non-binary\n",
        "Car Brands: Toyota, Ford, Honda\n",
        "Color: Red, Blue, Green\n",
        "Customer Satisfaction Levels: Satisfied, Neutral, Dissatisfied\n",
        "Comparison Table:\n",
        "Aspect\tContinuous Variable\tCategorical Variable\n",
        "Nature\tMeasurable numerical values\tLabels or groups\n",
        "Possible Values\tInfinite within a range\tFinite, distinct categories\n",
        "Examples\tHeight, weight, temperature\tGender, color, product type\n",
        "Operations\tArithmetic (e.g., average)\tCounting (e.g., frequency)\n",
        "Subtypes\tInterval, Ratio\tNominal, Ordinal\n",
        "Why the Distinction Matters:\n",
        "Model Selection:\n",
        "\n",
        "Some models work well with continuous data (e.g., Linear Regression), while others handle categorical data better (e.g., Decision Trees).\n",
        "Feature Engineering:\n",
        "\n",
        "Continuous data might be normalized or standardized.\n",
        "Categorical data may require encoding (e.g., One-Hot Encoding or Label Encoding).\n",
        "Visualization:\n",
        "\n",
        "Continuous variables can be visualized with histograms or scatter plots.\n",
        "Categorical variables are typically visualized with bar charts or pie charts."
      ],
      "metadata": {
        "id": "JV-byISPhxjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6 How do we handle categorical variables in Machine Learning? What are the common t echniques?"
      ],
      "metadata": {
        "id": "GDKqM2Wmh63m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans Handling Categorical Variables in Machine Learning\n",
        "In machine learning, most algorithms require numerical input. Since categorical variables are typically non-numerical (e.g., colors, genders, product categories), they need to be converted into numerical representations for use in machine learning models.\n",
        "\n",
        "Here are the common techniques for handling categorical variables:\n",
        "\n",
        "1. One-Hot Encoding\n",
        "Description:\n",
        "\n",
        "Converts each category into a binary column (0s and 1s).\n",
        "A new binary feature is created for each unique category in the original variable.\n",
        "Example:\n",
        "For a feature Color with categories Red, Blue, Green:\n",
        "\n",
        "Color\tRed\tBlue\tGreen\n",
        "Red\t1\t0\t0\n",
        "Blue\t0\t1\t0\n",
        "Green\t0\t0\t1\n",
        "When to Use:\n",
        "\n",
        "When the number of categories is small to moderate.\n",
        "For nominal variables (no inherent order).\n",
        "Pros:\n",
        "\n",
        "Simple and effective for many algorithms.\n",
        "Cons:\n",
        "\n",
        "Can lead to the curse of dimensionality if there are many unique categories.\n",
        "2. Label Encoding\n",
        "Description:\n",
        "\n",
        "Assigns a unique integer to each category.\n",
        "Categories are mapped to numbers (e.g., Red ‚Üí 0, Blue ‚Üí 1, Green ‚Üí 2).\n",
        "Example:\n",
        "\n",
        "Color\tLabel\n",
        "Red\t0\n",
        "Blue\t1\n",
        "Green\t2\n",
        "When to Use:\n",
        "\n",
        "When there are few categories.\n",
        "For ordinal variables (categories with an inherent order).\n",
        "Pros:\n",
        "\n",
        "Simple and space-efficient.\n",
        "Cons:\n",
        "\n",
        "Algorithms might interpret the encoded values as having an order or magnitude, which can lead to misleading results for nominal data.\n",
        "3. Ordinal Encoding\n",
        "Description:\n",
        "\n",
        "Similar to label encoding but specifically used when categories have a natural order.\n",
        "Example:\n",
        "For a feature Size with categories Small, Medium, Large:\n",
        "\n",
        "Size\tOrdinal Code\n",
        "Small\t1\n",
        "Medium\t2\n",
        "Large\t3\n",
        "When to Use:\n",
        "\n",
        "For ordinal variables where the order matters (e.g., rankings, satisfaction levels).\n",
        "Pros:\n",
        "\n",
        "Preserves the order information.\n",
        "Cons:\n",
        "\n",
        "Assumes the differences between categories are uniform, which might not always be the case.\n",
        "4. Frequency Encoding\n",
        "Description:\n",
        "\n",
        "Assigns a category's code based on the frequency of occurrences in the dataset.\n",
        "Example:\n",
        "\n",
        "City\tFrequency\tEncoded Value\n",
        "New York\t500\t500\n",
        "Los Angeles\t300\t300\n",
        "Chicago\t200\t200\n",
        "When to Use:\n",
        "\n",
        "For categorical variables with many categories.\n",
        "Pros:\n",
        "\n",
        "Handles high-cardinality data effectively.\n",
        "Cons:\n",
        "\n",
        "Frequency information might not capture relationships effectively.\n",
        "5. Target Encoding (Mean Encoding)\n",
        "Description:\n",
        "\n",
        "Replaces each category with the mean of the target variable (used in regression or classification tasks).\n",
        "Example:\n",
        "For a feature City predicting House Price:\n",
        "\n",
        "City\tAverage House Price\n",
        "New York\t700,000\n",
        "Los Angeles\t500,000\n",
        "Chicago\t300,000\n",
        "When to Use:\n",
        "\n",
        "When there's a strong correlation between the category and the target variable.\n",
        "Pros:\n",
        "\n",
        "Can capture target-specific patterns.\n",
        "Cons:\n",
        "\n",
        "Prone to overfitting if not handled carefully (e.g., use cross-validation).\n",
        "6. Binary Encoding\n",
        "Description:\n",
        "\n",
        "Combines aspects of label encoding and one-hot encoding by converting categories into binary codes.\n",
        "Example:\n",
        "For categories A, B, C, D:\n",
        "\n",
        "Category\tLabel\tBinary Code\tBinary Columns\n",
        "A\t1\t001\t0, 0, 1\n",
        "B\t2\t010\t0, 1, 0\n",
        "C\t3\t011\t0, 1, 1\n",
        "D\t4\t100\t1, 0, 0\n",
        "When to Use:\n",
        "\n",
        "When there are a moderate number of categories.\n",
        "Pros:\n",
        "\n",
        "Reduces dimensionality compared to one-hot encoding.\n",
        "Cons:\n",
        "\n",
        "More complex to implement.\n",
        "Choosing the Right Technique\n",
        "Scenario\tBest Technique\n",
        "Few categories, nominal data\tOne-Hot Encoding\n",
        "Few categories, ordinal data\tOrdinal Encoding\n",
        "Many categories\tFrequency Encoding, Binary Encoding\n",
        "Correlation with target variable\tTarget Encoding\n",
        "Minimal risk of order misinterpretation\tLabel Encoding\n",
        "Summary\n",
        "One-Hot Encoding and Label Encoding are the most common techniques.\n",
        "High-cardinality features (many categories) need more efficient encoding like Binary Encoding or Frequency Encoding.\n",
        "Be mindful of overfitting with techniques like Target Encoding."
      ],
      "metadata": {
        "id": "BvBJOqmCjIAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7 What do you mean by training and testing a dataset?"
      ],
      "metadata": {
        "id": "DbvY_ncAjqZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans Training and Testing a Dataset in Machine Learning\n",
        "In machine learning, datasets are typically divided into two main subsets: the training dataset and the testing dataset. This split is essential for building models that generalize well to unseen data.\n",
        "\n",
        "1. Training Dataset\n",
        "Definition:\n",
        "The training dataset is the portion of data used to train the machine learning model.\n",
        "The model learns patterns, relationships, and features from this dataset.\n",
        "It is used by algorithms to adjust internal parameters (like weights in a neural network) to minimize error during training.\n",
        "Key Points:\n",
        "Typically, 70% to 80% of the data is used for training.\n",
        "The model \"sees\" these examples during the training process.\n",
        "The training process involves minimizing a loss function to improve the model's performance.\n",
        "Example:\n",
        "For a dataset of house prices with features like Size, Location, and Price:\n",
        "\n",
        "Size (sq ft)\tLocation\tPrice ($)\n",
        "1500\tSuburb\t250,000\n",
        "2000\tUrban\t350,000\n",
        "1800\tSuburb\t280,000\n",
        "2. Testing Dataset\n",
        "Definition:\n",
        "The testing dataset is the portion of data used to evaluate the performance of the trained model.\n",
        "It acts as unseen data to check how well the model generalizes to new examples.\n",
        "It is never used during the training phase to avoid bias or overfitting.\n",
        "Key Points:\n",
        "Typically, 20% to 30% of the data is used for testing.\n",
        "Helps in understanding how the model performs on data it has not encountered before.\n",
        "The testing dataset is used to calculate performance metrics like accuracy, precision, recall, or mean squared error (MSE).\n",
        "Example:\n",
        "Continuing with the house price dataset:\n",
        "\n",
        "Size (sq ft)\tLocation\tPrice ($)\n",
        "1700\tUrban\t300,000\n",
        "2200\tSuburb\t400,000\n",
        "Why Split the Data?\n",
        "Prevent Overfitting:\n",
        "\n",
        "If the model only learns from the training data, it might memorize it and fail to generalize to new data. Testing helps identify overfitting.\n",
        "Evaluate Generalization:\n",
        "\n",
        "Testing on unseen data helps measure how well the model performs on real-world data.\n",
        "Model Validation:\n",
        "\n",
        "Ensures the model is robust and not biased toward the training data.\n",
        "Train-Test Split Ratio\n",
        "A common practice is to split the dataset in the following proportions:\n",
        "\n",
        "70% for Training and 30% for Testing (common default).\n",
        "80% for Training and 20% for Testing (when data is limited).\n",
        "60% for Training, 20% for Validation, and 20% for Testing (if using a validation set for tuning).\n",
        "Process Summary\n",
        "Training Phase:\n",
        "\n",
        "The model learns patterns from the training dataset by adjusting parameters to minimize errors (e.g., using Gradient Descent).\n",
        "Testing Phase:\n",
        "\n",
        "The model is evaluated on the testing dataset to assess how well it performs on new data.\n",
        "Example Workflow:\n",
        "Split the Data:"
      ],
      "metadata": {
        "id": "mOxkra7Wj63R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create sample data (replace with your actual data loading)\n",
        "data = pd.DataFrame\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create sample data (replace with your actual data loading)\n",
        "data = pd.DataFrame({\n",
        "    'Size': [1500, 2000, 1800, 1700, 2200],\n",
        "    'Location': ['Suburb', 'Urban', 'Suburb', 'Urban', 'Suburb'],\n",
        "    'Price': [250000, 350000, 280000, 320000, 400000]\n",
        "})"
      ],
      "metadata": {
        "id": "5OFqhISlmWFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the Model:"
      ],
      "metadata": {
        "id": "cTLqegudmu65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Create sample data (replace with your actual data loading)\n",
        "data = pd.DataFrame({\n",
        "    'Size': [1500, 2000, 1800, 1700, 2200],\n",
        "    'Location': ['Suburb', 'Urban', 'Suburb', 'Urban', 'Suburb'],\n",
        "    'Price': [250000, 350000, 280000, 320000, 400000]\n",
        "})\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = data[['Size', 'Location']]  # Features for prediction\n",
        "y = data['Price']  # Target variable\n",
        "\n",
        "# One-hot encode the 'Location' feature\n",
        "X = pd.get_dummies(X, columns=['Location'], drop_first=True) # Avoid dummy variable trap\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 80% train, 20% test, random_state for reproducibility\n",
        "\n",
        "# Create and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train) # Now X_train and y_train are defined"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "tCksM7wAnCTt",
        "outputId": "6a24d3df-fb9c-4dfc-e330-09e4f1c56371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"‚ñ∏\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"‚ñæ\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LinearRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LinearRegression.html\">?<span>Documentation for LinearRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LinearRegression()</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the Model:"
      ],
      "metadata": {
        "id": "BGSJDz4tnH55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "print(f\"Mean Squared Error: {mse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Byjqn3JnMpr",
        "outputId": "d22f5250-af45-41c2-aa7f-8ab4e2115663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 1272753834.9159975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Takeaways:\n",
        "Training Dataset: Used to build the model.\n",
        "Testing Dataset: Used to evaluate the model's performance.\n",
        "A good model performs well on both the training and testing datasets."
      ],
      "metadata": {
        "id": "_0haWttrnS27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8 What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "atv-uzb5nYqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans sklearn.preprocessing in Scikit-Learn\n",
        "sklearn.preprocessing is a module in the Scikit-Learn library (also known as sklearn) that provides various functions and classes for preprocessing data before it is fed into machine learning models. Preprocessing helps prepare data in a format that improves the performance and accuracy of models.\n",
        "\n",
        "Why Preprocessing is Important\n",
        "Scaling and Normalization: Many machine learning algorithms work better with scaled or normalized data.\n",
        "Encoding Categorical Variables: Converting non-numeric (categorical) data into numeric form so models can use it.\n",
        "Handling Missing Values: Replacing or imputing missing data.\n",
        "Improving Model Performance: Well-preprocessed data can help reduce training time and improve accuracy.\n",
        "Common Methods in sklearn.preprocessing\n",
        "Here are some of the most widely used preprocessing techniques available in the sklearn.preprocessing module:\n",
        "\n",
        "1. Standardization\n",
        "Standardization (or Z-score normalization) scales data so that it has a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "Function: StandardScaler"
      ],
      "metadata": {
        "id": "65djuuUFnjB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create sample data (replace with your actual data loading)\n",
        "data = pd.DataFrame({\n",
        "    'Size': [1500, 2000, 1800, 1700, 2200],\n",
        "    'Location': ['Suburb', 'Urban', 'Suburb', 'Urban', 'Suburb'],\n",
        "    'Price': [250000, 350000, 280000, 320000, 400000]\n",
        "})\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = data[['Size', 'Location']]  # Features for prediction\n",
        "y = data['Price']  # Target variable\n",
        "\n",
        "# One-hot encode the 'Location' feature\n",
        "X = pd.get_dummies(X, columns=['Location'], drop_first=True) # Avoid dummy variable trap\n",
        "\n",
        "# Now you can scale the numerical features only\n",
        "numerical_features = ['Size']  # List of numerical columns\n",
        "scaler = StandardScaler()\n",
        "X[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
        "\n",
        "# ... (rest of your code) ..."
      ],
      "metadata": {
        "id": "qs_d79oxnu8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example:\n",
        "Original Data\tScaled Data (Z-Score)\n",
        "10\t-1.25\n",
        "15\t0.00\n",
        "20\t1.25\n",
        "2. Min-Max Scaling\n",
        "Min-Max Scaling scales data to a fixed range, typically between 0 and 1.\n",
        "\n",
        "Function: MinMaxScaler\n",
        "python\n",
        "Copy code\n"
      ],
      "metadata": {
        "id": "8xSSs_CYnxz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler # Import MinMaxScaler\n",
        "\n",
        "# Create sample data (replace with your actual data loading)\n",
        "data = pd.DataFrame({\n",
        "    'Size': [1500, 2000, 1800, 1700, 2200],\n",
        "    'Location': ['Suburb', 'Urban', 'Suburb', 'Urban', 'Suburb'],\n",
        "    'Price': [250000, 350000, 280000, 320000, 400000]\n",
        "})\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = data[['Size', 'Location']]  # Features for prediction\n",
        "y = data['Price']  # Target variable\n",
        "\n",
        "# One-hot encode the 'Location' feature\n",
        "X = pd.get_dummies(X, columns=['Location'], drop_first=True) # Avoid dummy variable trap\n",
        "\n",
        "# Now you can scale the numerical features only\n",
        "numerical_features = ['Size']  # List of numerical columns\n",
        "\n",
        "# ---Changes start here---\n",
        "# 1. Separate numerical features for scaling\n",
        "numerical_data = data[numerical_features]\n",
        "\n",
        "# 2. Apply MinMaxScaler to numerical features only\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(numerical_data)\n",
        "\n",
        "# 3. Create a DataFrame from the scaled data\n",
        "scaled_data = pd.DataFrame(scaled_data, columns=numerical_features, index=data.index)\n",
        "\n",
        "# 4. Concatenate scaled numerical features with one-hot encoded categorical features\n",
        "X_scaled = pd.concat([scaled_data, X[['Location_Urban']]], axis=1)\n",
        "# ---Changes end here---\n",
        "\n",
        "# You can now use X_scaled for your model training\n",
        "# ... (rest of your code) ..."
      ],
      "metadata": {
        "id": "9TvqyptioF_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example:\n",
        "Original Data\tScaled Data (0 to 1)\n",
        "10\t0.0\n",
        "15\t0.5\n",
        "20\t1.0\n",
        "3. Normalization\n",
        "Normalization scales each data point by the magnitude of the entire feature vector, bringing all values to a unit norm (e.g., length 1).\n",
        "\n",
        "Function: Normalizer"
      ],
      "metadata": {
        "id": "uHm4L-9RoVk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer # Import Normalizer\n",
        "\n",
        "# Create sample data (replace with your actual data loading)\n",
        "data = pd.DataFrame({\n",
        "    'Size': [1500, 2000, 1800, 1700, 2200],\n",
        "    'Location': ['Suburb', 'Urban', 'Suburb', 'Urban', 'Suburb'],\n",
        "    'Price': [250000, 350000, 280000, 320000, 400000]\n",
        "})\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = data[['Size', 'Location']]  # Features for prediction\n",
        "y = data['Price']  # Target variable\n",
        "\n",
        "# One-hot encode the 'Location' feature\n",
        "X = pd.get_dummies(X, columns=['Location'], drop_first=True) # Avoid dummy variable trap\n",
        "\n",
        "# Now you can scale the numerical features only\n",
        "numerical_features = ['Size']  # List of numerical columns\n",
        "\n",
        "# Apply Normalizer to numerical features only\n",
        "normalizer = Normalizer()\n",
        "X_scaled = X.copy()  # Create a copy to avoid modifying original X\n",
        "X_scaled[numerical_features] = normalizer.fit_transform(X_scaled[numerical_features])\n",
        "\n",
        "# You can now use X_scaled for your model training\n",
        "# ..."
      ],
      "metadata": {
        "id": "X3nh65s1ofT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Case:\n",
        "Often used for text data or when the magnitude of vectors matters.\n",
        "\n",
        "4. Encoding Categorical Variables\n",
        "a. One-Hot Encoding\n",
        "Converts each category into a new binary (0 or 1) feature.\n",
        "\n",
        "Function: OneHotEncoder"
      ],
      "metadata": {
        "id": "d44ca40GojJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder()\n",
        "encoded_data = encoder.fit_transform(data)\n"
      ],
      "metadata": {
        "id": "aR98EsRFokvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example:\n",
        "Color\tRed\tBlue\tGreen\n",
        "Red\t1\t0\t0\n",
        "Blue\t0\t1\t0\n",
        "Green\t0\t0\t1\n",
        "b. Label Encoding\n",
        "Assigns a unique integer to each category.\n",
        "\n",
        "Function: LabelEncoder"
      ],
      "metadata": {
        "id": "h_1ohaKionKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoded_labels = encoder.fit_transform(data)\n"
      ],
      "metadata": {
        "id": "eaCZ0GfZotDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example:\n",
        "Color\tLabel\n",
        "Red\t0\n",
        "Blue\t1\n",
        "Green\t2\n",
        "5. Binarization\n",
        "Converts numeric data to binary (0 or 1) based on a threshold.\n",
        "\n",
        "Function: Binarizer"
      ],
      "metadata": {
        "id": "Z2f_l9rVovgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import Binarizer\n",
        "\n",
        "binarizer = Binarizer(threshold=10)\n",
        "binary_data = binarizer.fit_transform(data)\n"
      ],
      "metadata": {
        "id": "hgaJQQjToxg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example:\n",
        "Original Data\tBinarized Data\n",
        "5\t0\n",
        "15\t1\n",
        "6. Polynomial Feature Expansion\n",
        "Generates polynomial and interaction features.\n",
        "\n",
        "Function: PolynomialFeatures"
      ],
      "metadata": {
        "id": "EEqramWto0Pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "poly_features = poly.fit_transform(data)\n"
      ],
      "metadata": {
        "id": "S_jJocTBo2ZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Imputation (Handling Missing Values)\n",
        "Fills missing values with specified strategies like mean, median, or most frequent value.\n",
        "\n",
        "Function: SimpleImputer"
      ],
      "metadata": {
        "id": "mzRQDdzSo9ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "imputed_data = imputer.fit_transform(data)\n"
      ],
      "metadata": {
        "id": "xfRdJWMho_dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of Common Functions\n",
        "Method\tFunction\tUse Case\n",
        "Standardization\tStandardScaler\tScale to mean 0, standard deviation 1\n",
        "Min-Max Scaling\tMinMaxScaler\tScale between a specified range (e.g., 0 to 1)\n",
        "Normalization\tNormalizer\tScale feature vectors to unit norm\n",
        "One-Hot Encoding\tOneHotEncoder\tEncode nominal categorical data\n",
        "Label Encoding\tLabelEncoder\tEncode ordinal categorical data\n",
        "Binarization\tBinarizer\tThreshold-based binary conversion\n",
        "Polynomial Features\tPolynomialFeatures\tCreate polynomial feature expansions\n",
        "Imputation\tSimpleImputer\tHandle missing data\n",
        "Conclusion\n",
        "The sklearn.preprocessing module provides versatile tools for preprocessing data before feeding it into machine learning models. Proper preprocessing can lead to more accurate and robust models, helping to mitigate issues caused by data variability, categorical features, and missing values."
      ],
      "metadata": {
        "id": "RsaxV64GpC3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9 What is a Test set?"
      ],
      "metadata": {
        "id": "orarzFEop1NZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans Test Set in Machine Learning\n",
        "A test set is a portion of the dataset that is used to evaluate the performance of a machine learning model after it has been trained. It is crucial to assess how well the model generalizes to unseen data and provides an estimate of the model's performance on real-world data.\n",
        "\n",
        "Key Characteristics of a Test Set:\n",
        "Unseen Data:\n",
        "The test set is not used during the training phase. It is completely separate from the training data, which means the model has never seen this data before.\n",
        "\n",
        "Model Evaluation:\n",
        "The test set is used to evaluate the final model after it has been trained. This helps assess how well the model performs on unseen data, giving an indication of how it will perform on new, real-world data.\n",
        "\n",
        "Performance Metrics:\n",
        "After evaluating the model on the test set, performance metrics such as accuracy, precision, recall, F1-score, mean squared error (MSE), etc., are calculated to gauge the model's effectiveness.\n",
        "\n",
        "How the Test Set is Used:\n",
        "Split the Data:\n",
        "Typically, the dataset is split into a training set and a test set (often along the lines of 80-20 or 70-30), with the test set being the smaller portion.\n",
        "\n",
        "Train the Model:\n",
        "The model is trained on the training set, where it learns patterns from the data by adjusting its internal parameters.\n",
        "\n",
        "Evaluate on the Test Set:\n",
        "Once the model is trained, its performance is evaluated on the test set. This ensures the model is not overfitting to the training data and is capable of generalizing to new, unseen data.\n",
        "\n",
        "Why is the Test Set Important?\n",
        "Prevents Overfitting:\n",
        "By testing on data that the model has never seen before, the test set helps ensure that the model is not simply memorizing the training data, but is learning generalizable patterns.\n",
        "\n",
        "Measures Generalization:\n",
        "The performance on the test set gives an estimate of how well the model will perform in real-world situations with new data.\n",
        "\n",
        "Model Comparison:\n",
        "The test set is essential for comparing different models. By evaluating multiple models on the same test set, you can determine which one generalizes better.\n",
        "\n",
        "Example:\n",
        "Consider a dataset for predicting house prices, where you have features like Size, Location, and Number of Bedrooms, and the target is Price.\n",
        "\n",
        "Step 1: Split the Data"
      ],
      "metadata": {
        "id": "1gJJLbRFqBrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'data' is your DataFrame\n",
        "\n",
        "# 1. Verify column names:\n",
        "print(data.columns)  # Check if 'Bedrooms' (or a similar name) is in the columns\n",
        "\n",
        "# 2. If 'Bedrooms' is misspelled, correct it:\n",
        "X = data[['Size', 'Location', 'CorrectColumnName']]  # Replace 'CorrectColumnName'\n",
        "\n",
        "# 3. If 'Bedrooms' is missing, create it or load it from your data source:\n",
        "# For example, if you have a column 'BedRooms' instead of 'Bedrooms':\n",
        "data['Bedrooms'] = data['BedRoom']\n",
        "X = data[['Size', 'Location', 'Bedroom']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "EfYJkatTqNC6",
        "outputId": "d7dc28d3-b78a-4253-bf26-dff0c4fbda3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Size', 'Location', 'Price'], dtype='object')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['CorrectColumnName'] not in index\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-7f37e37d7a4f>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 2. If 'Bedrooms' is misspelled, correct it:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Location'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CorrectColumnName'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Replace 'CorrectColumnName'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 3. If 'Bedrooms' is missing, create it or load it from your data source:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6254\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['CorrectColumnName'] not in index\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Train the Model"
      ],
      "metadata": {
        "id": "ZqOwXj20qcAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "EiyhAP7oqdN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Evaluate on the Test Set"
      ],
      "metadata": {
        "id": "TNwQ9SkwqghM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "print(f\"Mean Squared Error on Test Set: {mse}\")\n"
      ],
      "metadata": {
        "id": "rKK8Dm2jqiIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "The test set is a separate subset of the dataset used to evaluate the model after training.\n",
        "It is important because it helps assess the model's generalization ability to unseen data.\n",
        "The test set helps prevent overfitting and ensures that the model performs well on real-world data."
      ],
      "metadata": {
        "id": "CIpNliWTqj8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10 How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?"
      ],
      "metadata": {
        "id": "0Ja4wCF-qn2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans How to Split Data for Model Fitting (Training and Testing) in Python\n",
        "In Python, you can split your dataset into training and testing sets using the train_test_split function from the sklearn.model_selection module. This split allows you to train your model on one subset (training set) and evaluate its performance on another, unseen subset (test set).\n",
        "\n",
        "1. Data Splitting in Python using train_test_split\n",
        "The train_test_split function randomly splits data into two subsets: one for training the model and the other for testing its performance.\n",
        "\n",
        "Steps to Split Data:\n",
        "Import the necessary libraries:"
      ],
      "metadata": {
        "id": "mH4KAgoorSvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "FnMLCuVwrVJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare your features (X) and target variable (y):\n",
        "\n",
        "X: Features or input variables.\n",
        "y: Target variable (what you want to predict).\n",
        "Example:"
      ],
      "metadata": {
        "id": "isWvpzB4rZAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data[['Feature1', 'Feature2', 'Feature3']]\n",
        "y = data['Target']\n"
      ],
      "metadata": {
        "id": "WmIFXmv5rar2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use train_test_split to split the data:\n",
        "\n",
        "You can specify the proportion of data you want for the test set (commonly 20%).\n",
        "You can also set a random_state for reproducibility.\n",
        "Example:"
      ],
      "metadata": {
        "id": "ehYKFoZzreih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "OjWqGvp6rggF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test_size=0.2 means that 20% of the data will be used for testing, and 80% will be used for training.\n",
        "random_state=42 ensures that the split is reproducible.\n",
        "Example Code for Data Splitting:"
      ],
      "metadata": {
        "id": "8z7pM1iYrkHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Sample dataset\n",
        "data = pd.DataFrame({\n",
        "    'Feature1': [1, 2, 3, 4, 5],\n",
        "    'Feature2': [5, 4, 3, 2, 1],\n",
        "    'Target': [10, 20, 30, 40, 50]\n",
        "})\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = data[['Feature1', 'Feature2']]\n",
        "y = data['Target']\n",
        "\n",
        "# Split data into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the resulting datasets\n",
        "print(\"Training Features Shape:\", X_train.shape)\n",
        "print(\"Testing Features Shape:\", X_test.shape)\n",
        "print(\"Training Target Shape:\", y_train.shape)\n",
        "print(\"Testing Target Shape:\", y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2UtDYGYrmsu",
        "outputId": "f5f3e0a5-d83a-4212-c7d4-15f90dab145f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Features Shape: (4, 2)\n",
            "Testing Features Shape: (1, 2)\n",
            "Training Target Shape: (4,)\n",
            "Testing Target Shape: (1,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How Do You Approach a Machine Learning Problem?\n",
        "Approaching a machine learning problem involves a series of steps that guide you from understanding the problem to deploying a model. Below is a structured way to approach a typical machine learning task:\n",
        "\n",
        "1. Define the Problem\n",
        "Understand the Problem Statement:\n",
        "Understand what you are trying to predict or classify. Are you solving a regression (predicting continuous values) or classification (predicting discrete categories) problem?\n",
        "Clarify the Objective:\n",
        "Define the business or research objective clearly to understand how the model's success will be evaluated.\n",
        "2. Collect and Explore the Data\n",
        "Data Collection:\n",
        "Gather relevant datasets. These could be available datasets, or you might need to collect your own data.\n",
        "\n",
        "Data Exploration (EDA):\n",
        "Perform Exploratory Data Analysis (EDA) to understand the data:\n",
        "\n",
        "Check for missing values.\n",
        "Understand the distribution of the features.\n",
        "Visualize the data to uncover patterns and correlations.\n",
        "Example tasks:\n",
        "\n",
        "Use describe() to get a summary of the data.\n",
        "Use visualizations like histograms, box plots, and scatter plots.\n",
        "Example:"
      ],
      "metadata": {
        "id": "4VvBDdW8rrLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()  # Summary statistics\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "PVPUYoMLrs73",
        "outputId": "b06586aa-a9a3-47d2-e4d4-cac03bdaa7b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Feature1  Feature2     Target\n",
              "count  5.000000  5.000000   5.000000\n",
              "mean   3.000000  3.000000  30.000000\n",
              "std    1.581139  1.581139  15.811388\n",
              "min    1.000000  1.000000  10.000000\n",
              "25%    2.000000  2.000000  20.000000\n",
              "50%    3.000000  3.000000  30.000000\n",
              "75%    4.000000  4.000000  40.000000\n",
              "max    5.000000  5.000000  50.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ff97a3df-c3d6-421a-97de-269bedbc576a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Feature1</th>\n",
              "      <th>Feature2</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>30.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.581139</td>\n",
              "      <td>1.581139</td>\n",
              "      <td>15.811388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>10.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>20.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>30.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>40.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>50.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ff97a3df-c3d6-421a-97de-269bedbc576a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ff97a3df-c3d6-421a-97de-269bedbc576a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ff97a3df-c3d6-421a-97de-269bedbc576a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e36df1a2-bd76-4b46-81cf-a775ecb0a212\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e36df1a2-bd76-4b46-81cf-a775ecb0a212')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e36df1a2-bd76-4b46-81cf-a775ecb0a212 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Feature1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.5104848666672712,\n        \"min\": 1.0,\n        \"max\": 5.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          5.0,\n          3.0,\n          4.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Feature2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.5104848666672712,\n        \"min\": 1.0,\n        \"max\": 5.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          5.0,\n          3.0,\n          4.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 15.27999762100796,\n        \"min\": 5.0,\n        \"max\": 50.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          5.0,\n          30.0,\n          40.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Data Preprocessing\n",
        "Handle Missing Values:\n",
        "Use techniques like mean imputation or forward filling to handle missing values.\n",
        "\n",
        "Feature Scaling:\n",
        "Scale numerical features using techniques like Standardization or Min-Max scaling to ensure they are on the same scale.\n",
        "\n",
        "Encoding Categorical Data:\n",
        "Convert categorical features into numerical ones using Label Encoding or One-Hot Encoding.\n",
        "\n",
        "Split Data:\n",
        "Split the data into training and testing sets using train_test_split.\n",
        "\n",
        "4. Choose a Model\n",
        "Based on the problem (regression or classification), choose an appropriate model. For example:\n",
        "Regression: Linear Regression, Decision Trees, Random Forests.\n",
        "Classification: Logistic Regression, k-NN, SVM, Random Forests.\n",
        "Example:"
      ],
      "metadata": {
        "id": "H5qUoNTMrxZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n"
      ],
      "metadata": {
        "id": "tT0mgJhFr5JE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Train the Model\n",
        "Fit the Model:\n",
        "Train the model on the training data using the fit() method.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "kCJ2J-kDr7mX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "z2nzCAzDr9n2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Evaluate the Model\n",
        "Testing on Unseen Data:\n",
        "After training, test the model on the test set to evaluate its performance.\n",
        "\n",
        "Metrics to Evaluate:\n",
        "\n",
        "For classification: Accuracy, Precision, Recall, F1-Score, ROC-AUC.\n",
        "For regression: Mean Squared Error (MSE), R-squared.\n",
        "Example:"
      ],
      "metadata": {
        "id": "39PLT6kcr_wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "uWwVOhIusBuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Model Tuning\n",
        "Hyperparameter Tuning:\n",
        "Tune the model's hyperparameters to improve performance. Use techniques like Grid Search or Randomized Search to find the best set of hyperparameters.\n",
        "\n",
        "Example (Grid Search):"
      ],
      "metadata": {
        "id": "0SYUpRCysD_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC # Import the SVC class\n",
        "\n",
        "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "OeUQFYSAsWoK",
        "outputId": "5a6b4bd1-5495-4188-8b1f-8fee0d556346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot have number of splits n_splits=5 greater than the number of samples: n_samples=4.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-78eec88da037>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'kernel'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rbf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    975\u001b[0m                         \u001b[0;34m**\u001b[0m\u001b[0mfit_and_score_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m                     )\n\u001b[0;32m--> 977\u001b[0;31m                     for (cand_idx, parameters), (split_idx, (train, test)) in product(\n\u001b[0m\u001b[1;32m    978\u001b[0m                         \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m                         \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrouted_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    410\u001b[0m                 (\n\u001b[1;32m    411\u001b[0m                     \u001b[0;34m\"Cannot have number of splits n_splits={0} greater\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot have number of splits n_splits=5 greater than the number of samples: n_samples=4."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Model Validation\n",
        "Cross-Validation:\n",
        "Use k-fold cross-validation to validate the model‚Äôs performance and reduce variance caused by a single train-test split.\n",
        "9. Model Deployment\n",
        "Once you are satisfied with the model's performance, deploy it into production (e.g., API, web service) where it can make predictions on new data.\n",
        "Summary of the Approach:\n",
        "Define the problem (regression or classification).\n",
        "Collect and explore data using EDA.\n",
        "Preprocess the data (handle missing values, scale features, encode categories).\n",
        "Split the data into training and testing sets.\n",
        "Choose and train a model.\n",
        "Evaluate the model on test data using performance metrics.\n",
        "Tune the model using techniques like hyperparameter tuning or cross-validation.\n",
        "Deploy the model for real-world use.\n",
        "By following this approach, you can systematically solve machine learning problems and build effective models."
      ],
      "metadata": {
        "id": "v4WZiLF2sIx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11 Why do we have to perform EDA before fitting a model to the data?"
      ],
      "metadata": {
        "id": "6oef7ttVsc-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans Performing Exploratory Data Analysis (EDA) before fitting a machine learning model to the data is crucial for several reasons. EDA helps you understand the data better, which can significantly improve the performance of your model. Here's why it's an essential step in the machine learning workflow:\n",
        "\n",
        "1. Understanding the Structure of the Data\n",
        "Check for missing values:\n",
        "EDA allows you to identify missing data, which can significantly impact the performance of a model. You can decide whether to impute the missing values, drop rows/columns, or use other techniques to handle them.\n",
        "\n",
        "Examine the distribution of features:\n",
        "By visualizing the distributions of features (e.g., histograms, box plots), you can understand whether any feature is skewed or requires transformations (e.g., normalization or standardization) before being used in a model.\n",
        "\n",
        "Identify outliers:\n",
        "Outliers can heavily influence some machine learning models (like linear regression or k-NN). By identifying outliers during EDA, you can decide whether to handle them by removing or transforming them.\n",
        "\n",
        "2. Identifying Data Types and Relationships\n",
        "Categorical vs. numerical data:\n",
        "EDA helps you distinguish between categorical and numerical features. This is important because categorical variables often need encoding (e.g., one-hot encoding or label encoding) to be used in models.\n",
        "\n",
        "Feature relationships and correlations:\n",
        "By visualizing pairwise relationships (e.g., using scatter plots or correlation matrices), you can detect correlations between features, which may help in feature selection. Highly correlated features might need to be removed to avoid multicollinearity in models like linear regression or logistic regression.\n",
        "\n",
        "3. Detecting Data Quality Issues\n",
        "Inconsistent or incorrect data:\n",
        "EDA helps in identifying data quality issues, such as typos, incorrect entries, or inconsistent formatting. Fixing these issues before model fitting ensures that the model is not misled by incorrect or irrelevant data.\n",
        "\n",
        "Data scaling issues:\n",
        "If you have features with very different scales (e.g., one feature ranges from 1 to 1000, while another ranges from 0 to 1), EDA can reveal these disparities. Scaling might be necessary for algorithms like k-NN, SVM, or neural networks, which are sensitive to the magnitude of features.\n",
        "\n",
        "4. Feature Engineering and Selection\n",
        "Creating new features:\n",
        "During EDA, you might discover opportunities to create new features that can improve model performance. For example, by combining or transforming existing features (e.g., creating a total_spent feature from price and quantity).\n",
        "\n",
        "Identifying irrelevant features:\n",
        "Some features might not provide useful information for the model. EDA helps you identify and remove redundant or irrelevant features, improving model efficiency and interpretability.\n",
        "\n",
        "5. Understanding Data Imbalance\n",
        "Class imbalance in classification tasks:\n",
        "In classification problems, EDA helps you visualize the distribution of the target variable (e.g., using bar plots). If the classes are imbalanced (e.g., 90% of data points belong to one class), you can take steps to address the imbalance, such as resampling the dataset, using different evaluation metrics, or applying techniques like SMOTE (Synthetic Minority Over-sampling Technique).\n",
        "6. Hypothesis Generation\n",
        "Understanding patterns and trends:\n",
        "EDA helps to generate hypotheses about the data. For example, you might notice that a specific feature has a strong relationship with the target variable. This insight can guide your choice of model or feature engineering.\n",
        "7. Selecting the Right Model\n",
        "Model assumptions:\n",
        "Many machine learning algorithms have assumptions about the data (e.g., linearity in linear regression, normality in logistic regression). EDA helps you understand if the data meets those assumptions or if data transformation (such as logarithmic transformations or polynomial features) is needed.\n",
        "\n",
        "Choosing the right model:\n",
        "By understanding the data through EDA, you can choose the most appropriate machine learning algorithm. For example, if you find that relationships between variables are non-linear, you may opt for tree-based models (like Random Forests) or neural networks instead of linear models.\n",
        "\n",
        "Example of EDA Steps Before Model Fitting\n",
        "Data Inspection:\n",
        "\n",
        "View the first few rows of the data using head() to get an overview.\n",
        "Check the data types using info() to identify categorical and numerical variables.\n",
        "Handling Missing Values:\n",
        "\n",
        "Identify missing values with isnull() or sum() to decide how to handle them.\n",
        "Statistical Summary:\n",
        "\n",
        "Use describe() to get summary statistics for numerical features.\n",
        "Data Visualization:\n",
        "\n",
        "Use histograms or box plots to visualize distributions.\n",
        "Use scatter plots or correlation matrices to explore relationships between features.\n",
        "Feature Correlation:\n",
        "\n",
        "Use a heatmap to visualize correlations between features and target variables.\n",
        "Outlier Detection:\n",
        "\n",
        "Visualize data distributions to spot outliers (e.g., using box plots) and decide on how to handle them.\n",
        "Summary of Why EDA is Important:\n",
        "Understand your data: You can assess the structure, distribution, and relationships between features.\n",
        "Identify issues: EDA helps identify missing values, outliers, and data quality problems that could impact model performance.\n",
        "Feature selection and engineering: EDA allows you to select relevant features and create new ones that can improve your model.\n",
        "Prepare data for modeling: It helps in transforming data, handling imbalances, and deciding which machine learning algorithms are most appropriate.\n",
        "EDA is a vital first step because it helps you clean, transform, and structure the data in ways that make the model-building process more efficient, accurate, and interpretable."
      ],
      "metadata": {
        "id": "Z2eJkyDRspE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12 What is correlation?\n"
      ],
      "metadata": {
        "id": "VWb9IEcUswNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans Correlation is a statistical measure that describes the degree to which two variables move in relation to each other. It quantifies the strength and direction of the relationship between these variables.\n",
        "\n",
        "Types of Correlation\n",
        "Positive Correlation:\n",
        "\n",
        "When one variable increases, the other also increases.\n",
        "Example: Height and weight typically have a positive correlation.\n",
        "Negative Correlation:\n",
        "\n",
        "When one variable increases, the other decreases.\n",
        "Example: The more time spent exercising, the lower body fat percentage tends to be.\n",
        "Zero (No) Correlation:\n",
        "\n",
        "No discernible relationship between the two variables.\n",
        "Example: Shoe size and intelligence.\n",
        "Correlation Coefficient\n",
        "The most common measure is the Pearson correlation coefficient (\n",
        "ùëü\n",
        "r), which ranges between -1 and 1:\n",
        "\n",
        "ùëü\n",
        "=\n",
        "1\n",
        "r=1: Perfect positive correlation.\n",
        "ùëü\n",
        "=\n",
        "‚àí\n",
        "1\n",
        "r=‚àí1: Perfect negative correlation.\n",
        "ùëü\n",
        "=\n",
        "0\n",
        "r=0: No correlation.\n",
        "Formula for Pearson Correlation Coefficient:\n",
        "ùëü\n",
        "=\n",
        "‚àë\n",
        "(\n",
        "ùë•\n",
        "ùëñ\n",
        "‚àí\n",
        "ùë•\n",
        "Àâ\n",
        ")\n",
        "(\n",
        "ùë¶\n",
        "ùëñ\n",
        "‚àí\n",
        "ùë¶\n",
        "Àâ\n",
        ")\n",
        "‚àë\n",
        "(\n",
        "ùë•\n",
        "ùëñ\n",
        "‚àí\n",
        "ùë•\n",
        "Àâ\n",
        ")\n",
        "2\n",
        "‚àë\n",
        "(\n",
        "ùë¶\n",
        "ùëñ\n",
        "‚àí\n",
        "ùë¶\n",
        "Àâ\n",
        ")\n",
        "2\n",
        "r=\n",
        "‚àë(x\n",
        "i\n",
        "‚Äã\n",
        " ‚àí\n",
        "x\n",
        "Àâ\n",
        " )\n",
        "2\n",
        " ‚àë(y\n",
        "i\n",
        "‚Äã\n",
        " ‚àí\n",
        "y\n",
        "Àâ\n",
        "‚Äã\n",
        " )\n",
        "2\n",
        "\n",
        "‚Äã\n",
        "\n",
        "‚àë(x\n",
        "i\n",
        "‚Äã\n",
        " ‚àí\n",
        "x\n",
        "Àâ\n",
        " )(y\n",
        "i\n",
        "‚Äã\n",
        " ‚àí\n",
        "y\n",
        "Àâ\n",
        "‚Äã\n",
        " )\n",
        "‚Äã\n",
        "\n",
        "Where:\n",
        "\n",
        "ùë•\n",
        "ùëñ\n",
        "x\n",
        "i\n",
        "‚Äã\n",
        "  and\n",
        "ùë¶\n",
        "ùëñ\n",
        "y\n",
        "i\n",
        "‚Äã\n",
        "  are the data points,\n",
        "ùë•\n",
        "Àâ\n",
        "x\n",
        "Àâ\n",
        "  is the mean of\n",
        "ùë•\n",
        "x,\n",
        "ùë¶\n",
        "Àâ\n",
        "y\n",
        "Àâ\n",
        "‚Äã\n",
        "  is the mean of\n",
        "ùë¶\n",
        "y.\n",
        "Important Notes:\n",
        "Correlation ‚â† Causation: A high correlation between two variables does not imply that one causes the other.\n",
        "Types of Correlation Methods: Besides Pearson's correlation, there are other methods like Spearman's rank correlation and Kendall's tau.\n",
        "Applications:\n",
        "In finance, to assess the relationship between the returns of two assets.\n",
        "In science, to understand how variables might relate (e.g., temperature and ice cream sales)."
      ],
      "metadata": {
        "id": "_RbWL1wqAttk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13 What does negative correlation mean?"
      ],
      "metadata": {
        "id": "b6Ui0lmmBAhF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans A negative correlation means that as one variable increases, the other decreases. In other words, the two variables move in opposite directions.\n",
        "\n",
        "Key Points:\n",
        "Inverse Relationship: When one variable goes up, the other tends to go down.\n",
        "The correlation coefficient (\n",
        "ùëü\n",
        "r) for a negative correlation lies between 0 and -1.\n",
        "ùëü\n",
        "=\n",
        "‚àí\n",
        "1\n",
        "r=‚àí1 represents a perfect negative correlation.\n",
        "ùëü\n",
        "=\n",
        "0\n",
        "r=0 represents no correlation.\n",
        "Examples of Negative Correlation:\n",
        "Exercise and Body Fat:\n",
        "\n",
        "The more time spent exercising, the lower a person's body fat percentage tends to be.\n",
        "Car Speed and Travel Time:\n",
        "\n",
        "The faster you drive (up to a point), the less time it takes to reach your destination.\n",
        "Demand and Price:\n",
        "\n",
        "For many products, when price increases, demand decreases.\n",
        "Visual Representation:\n",
        "In a scatter plot, a negative correlation typically shows points sloping downward from left to right.\n",
        "\n",
        "Important Note:\n",
        "A stronger negative correlation (closer to -1) indicates a more consistent inverse relationship.\n",
        "Correlation does not imply causation: Just because two variables are negatively correlated does not mean one causes the other to change."
      ],
      "metadata": {
        "id": "1o7azM3QBDfq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14 How can you find correlation between variables in Python?"
      ],
      "metadata": {
        "id": "OLcTigWJBGez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans You can easily find the correlation between variables in Python using libraries like pandas, numpy, and scipy. Here's a step-by-step guide for different methods:\n",
        "\n",
        "1. Using pandas\n",
        "The pandas library provides the DataFrame.corr() method to calculate correlations.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "_pqpFsF2BNy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'Height': [150, 160, 170, 180, 190],\n",
        "    'Weight': [50, 60, 70, 80, 90]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate correlation matrix\n",
        "correlation = df.corr()\n",
        "\n",
        "print(correlation)\n"
      ],
      "metadata": {
        "id": "YrCboriuBR36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output:"
      ],
      "metadata": {
        "id": "Hj8_JKsHBUcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "          Height    Weight\n",
        "Height     1.000     1.000\n",
        "Weight     1.000     1.000\n"
      ],
      "metadata": {
        "id": "shAzZpdiBWuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Options for method parameter in corr():\n",
        "method='pearson' (default): For linear correlation.\n",
        "method='spearman': For rank-based correlation.\n",
        "method='kendall': For Kendall‚Äôs tau correlation.\n",
        "2. Using numpy\n",
        "The numpy.corrcoef() function computes the Pearson correlation coefficient.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "qFjeBVYyBasL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "height = [150, 160, 170, 180, 190]\n",
        "weight = [50, 60, 70, 80, 90]\n",
        "\n",
        "correlation = np.corrcoef(height, weight)\n",
        "\n",
        "print(correlation)\n"
      ],
      "metadata": {
        "id": "yk03WIVoBcrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output:"
      ],
      "metadata": {
        "id": "tQppcGKQBiJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[[1. 1.]\n",
        " [1. 1.]]\n"
      ],
      "metadata": {
        "id": "bXpzrJxqBmNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is a correlation matrix where:\n",
        "\n",
        "correlation[0, 1] or correlation[1, 0] gives the correlation coefficient.\n",
        "3. Using scipy\n",
        "The scipy.stats module provides various correlation functions like pearsonr and spearmanr.\n",
        "\n",
        "Example for Pearson Correlation:"
      ],
      "metadata": {
        "id": "iivU7pFTBo6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "height = [150, 160, 170, 180, 190]\n",
        "weight = [50, 60, 70, 80, 90]\n",
        "\n",
        "correlation, p_value = pearsonr(height, weight)\n",
        "\n",
        "print(f\"Correlation: {correlation}\")\n",
        "print(f\"P-value: {p_value}\")\n"
      ],
      "metadata": {
        "id": "ziV8-xXOBrCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output:"
      ],
      "metadata": {
        "id": "EB2NaagdBtGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Correlation: 1.0\n",
        "P-value: 0.0\n"
      ],
      "metadata": {
        "id": "xcF7ZRLtBu8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The p-value helps test the significance of the correlation.\n",
        "\n",
        "4. Correlation Heatmap (Visualization)\n",
        "Using seaborn to visualize correlations as a heatmap:"
      ],
      "metadata": {
        "id": "Zvq4pj9cBxME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'A': [1, 2, 3, 4, 5],\n",
        "    'B': [5, 4, 3, 2, 1],\n",
        "    'C': [2, 3, 4, 5, 6]\n",
        "})\n",
        "\n",
        "# Plot correlation heatmap\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bvUV9CeKBzGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These methods make it easy to find and interpret correlations in your dataset!"
      ],
      "metadata": {
        "id": "9UtXelZPB1TO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15 What is causation? Explain difference between correlation and causation with an example."
      ],
      "metadata": {
        "id": "RrcjgH5hB3F9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans What is Causation?\n",
        "Causation (or causality) refers to a relationship where one variable directly influences or causes a change in another variable. In other words, a change in one variable leads to a change in the other.\n",
        "\n",
        "Example:\n",
        "Eating more calories causes weight gain.\n",
        "In this case, increasing caloric intake directly leads to an increase in body weight.\n",
        "Correlation vs. Causation\n",
        "Aspect\tCorrelation\tCausation\n",
        "Definition\tA statistical relationship between two variables.\tA direct cause-and-effect relationship.\n",
        "Direction\tVariables move together (positively or negatively).\tOne variable's change leads to another variable's change.\n",
        "Implication\tSuggests association, not influence.\tImplies that one variable influences the other.\n",
        "Strength of Proof\tEasier to calculate statistically.\tRequires evidence of a direct mechanism or experiment.\n",
        "Example to Illustrate the Difference\n",
        "Correlation Example:\n",
        "There is a positive correlation between ice cream sales and drowning incidents.\n",
        "\n",
        "This doesn't mean eating ice cream causes drowning.\n",
        "The underlying factor (a third variable) is hot weather, which increases both ice cream consumption and swimming, thereby increasing the risk of drowning.\n",
        "Causation Example:\n",
        "Smoking causes an increased risk of lung cancer.\n",
        "\n",
        "Extensive research shows that the chemicals in cigarette smoke damage lung tissues, directly leading to cancer.\n",
        "Key Points to Remember:\n",
        "Correlation ‚â† Causation: Just because two variables are correlated doesn‚Äôt mean one causes the other.\n",
        "Spurious Correlations: Sometimes two variables seem correlated due to coincidence or a hidden third variable.\n",
        "Establishing Causation: Requires controlled experiments, longitudinal studies, or clear evidence of a cause-and-effect mechanism."
      ],
      "metadata": {
        "id": "NHk5-MfgCCwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16 What is an Optimizer? What are different types of optimizers? Explain each with an example."
      ],
      "metadata": {
        "id": "mgRgB2V1CHgy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans What is an Optimizer?\n",
        "An optimizer is an algorithm used in machine learning and deep learning to adjust the model's parameters (like weights and biases) during training to minimize the loss function and improve accuracy. The goal of the optimizer is to find the optimal values for the parameters that reduce the error between the predicted and actual outputs.\n",
        "\n",
        "Optimizers are crucial in gradient-based learning methods like neural networks, where they help adjust weights based on gradients computed during backpropagation.\n",
        "\n",
        "Types of Optimizers\n",
        "Here are commonly used optimizers in machine learning and deep learning, with explanations and examples:\n",
        "\n",
        "1. Gradient Descent (GD)\n",
        "Description:\n",
        "The simplest optimizer that updates model parameters by taking steps proportional to the negative gradient of the loss function with respect to the parameters.\n",
        "\n",
        "Update Rule:\n",
        "\n",
        "ùúÉ\n",
        "=\n",
        "ùúÉ\n",
        "‚àí\n",
        "ùúÇ\n",
        "‚àá\n",
        "ùúÉ\n",
        "ùêΩ\n",
        "(\n",
        "ùúÉ\n",
        ")\n",
        "Œ∏=Œ∏‚àíŒ∑‚àá\n",
        "Œ∏\n",
        "‚Äã\n",
        " J(Œ∏)\n",
        "ùúÉ\n",
        "Œ∏: Model parameters (weights)\n",
        "ùúÇ\n",
        "Œ∑: Learning rate (step size)\n",
        "‚àá\n",
        "ùúÉ\n",
        "ùêΩ\n",
        "(\n",
        "ùúÉ\n",
        ")\n",
        "‚àá\n",
        "Œ∏\n",
        "‚Äã\n",
        " J(Œ∏): Gradient of the loss function\n",
        "ùêΩ\n",
        "J with respect to\n",
        "ùúÉ\n",
        "Œ∏\n",
        "Example:"
      ],
      "metadata": {
        "id": "MW3QlzqUCOgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pseudocode for basic gradient descent\n",
        "for epoch in range(num_epochs):\n",
        "    gradient = compute_gradient(loss_function, weights)\n",
        "    weights = weights - learning_rate * gradient\n"
      ],
      "metadata": {
        "id": "4cWlx9saCRUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pros:\n",
        "\n",
        "Simple and effective for small datasets.\n",
        "Cons:\n",
        "\n",
        "Computationally expensive for large datasets (needs full dataset for each update).\n",
        "2. Stochastic Gradient Descent (SGD)\n",
        "Description:\n",
        "Instead of computing gradients for the whole dataset, SGD updates parameters using gradients from a single random data point (or a small batch).\n",
        "\n",
        "Update Rule:\n",
        "\n",
        "ùúÉ\n",
        "=\n",
        "ùúÉ\n",
        "‚àí\n",
        "ùúÇ\n",
        "‚àá\n",
        "ùúÉ\n",
        "ùêΩ\n",
        "(\n",
        "ùúÉ\n",
        ";\n",
        "ùë•\n",
        "ùëñ\n",
        ",\n",
        "ùë¶\n",
        "ùëñ\n",
        ")\n",
        "Œ∏=Œ∏‚àíŒ∑‚àá\n",
        "Œ∏\n",
        "‚Äã\n",
        " J(Œ∏;x\n",
        "i\n",
        "‚Äã\n",
        " ,y\n",
        "i\n",
        "‚Äã\n",
        " )\n",
        "Example:"
      ],
      "metadata": {
        "id": "jS-Dn3qVCVjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for x_i, y_i in data_loader:  # iterate through batches\n",
        "        gradient = compute_gradient(loss_function, weights, x_i, y_i)\n",
        "        weights = weights - learning_rate * gradient\n"
      ],
      "metadata": {
        "id": "WrPftjYtCXzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pros:\n",
        "\n",
        "Faster updates, suitable for large datasets.\n",
        "Cons:\n",
        "\n",
        "Noisy updates can lead to instability.\n",
        "3. Momentum\n",
        "Description:\n",
        "Momentum helps accelerate gradient descent by adding a fraction of the previous update to the current update, reducing oscillations and improving convergence speed.\n",
        "\n",
        "Update Rule:\n",
        "\n",
        "ùë£\n",
        "ùë°\n",
        "=\n",
        "ùõΩ\n",
        "ùë£\n",
        "ùë°\n",
        "‚àí\n",
        "1\n",
        "+\n",
        "ùúÇ\n",
        "‚àá\n",
        "ùúÉ\n",
        "ùêΩ\n",
        "(\n",
        "ùúÉ\n",
        ")\n",
        "v\n",
        "t\n",
        "‚Äã\n",
        " =Œ≤v\n",
        "t‚àí1\n",
        "‚Äã\n",
        " +Œ∑‚àá\n",
        "Œ∏\n",
        "‚Äã\n",
        " J(Œ∏)\n",
        "ùúÉ\n",
        "=\n",
        "ùúÉ\n",
        "‚àí\n",
        "ùë£\n",
        "ùë°\n",
        "Œ∏=Œ∏‚àív\n",
        "t\n",
        "‚Äã\n",
        "\n",
        "ùë£\n",
        "ùë°\n",
        "v\n",
        "t\n",
        "‚Äã\n",
        " : Velocity (momentum term)\n",
        "ùõΩ\n",
        "Œ≤: Momentum coefficient (e.g., 0.9)\n",
        "Example:"
      ],
      "metadata": {
        "id": "H9axiYrJCahz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "v = 0\n",
        "beta = 0.9\n",
        "for epoch in range(num_epochs):\n",
        "    gradient = compute_gradient(loss_function, weights)\n",
        "    v = beta * v + learning_rate * gradient\n",
        "    weights = weights - v\n"
      ],
      "metadata": {
        "id": "kURLDWAjCf-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pros:\n",
        "\n",
        "Helps avoid local minima and speeds up convergence.\n",
        "Cons:\n",
        "\n",
        "Requires tuning of the momentum parameter\n",
        "ùõΩ\n",
        "Œ≤.\n",
        "4. RMSProp (Root Mean Square Propagation)\n",
        "Description:\n",
        "RMSProp adapts the learning rate by maintaining a running average of the square of the gradients to address issues like vanishing or exploding gradients.\n",
        "\n",
        "Update Rule:\n",
        "\n",
        "ùë†\n",
        "ùë°\n",
        "=\n",
        "ùõΩ\n",
        "ùë†\n",
        "ùë°\n",
        "‚àí\n",
        "1\n",
        "+\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùõΩ\n",
        ")\n",
        "(\n",
        "‚àá\n",
        "ùúÉ\n",
        "ùêΩ\n",
        "(\n",
        "ùúÉ\n",
        ")\n",
        ")\n",
        "2\n",
        "s\n",
        "t\n",
        "‚Äã\n",
        " =Œ≤s\n",
        "t‚àí1\n",
        "‚Äã\n",
        " +(1‚àíŒ≤)(‚àá\n",
        "Œ∏\n",
        "‚Äã\n",
        " J(Œ∏))\n",
        "2\n",
        "\n",
        "ùúÉ\n",
        "=\n",
        "ùúÉ\n",
        "‚àí\n",
        "ùúÇ\n",
        "ùë†\n",
        "ùë°\n",
        "+\n",
        "ùúñ\n",
        "‚àá\n",
        "ùúÉ\n",
        "ùêΩ\n",
        "(\n",
        "ùúÉ\n",
        ")\n",
        "Œ∏=Œ∏‚àí\n",
        "s\n",
        "t\n",
        "‚Äã\n",
        " +œµ\n",
        "‚Äã\n",
        "\n",
        "Œ∑\n",
        "‚Äã\n",
        " ‚àá\n",
        "Œ∏\n",
        "‚Äã\n",
        " J(Œ∏)\n",
        "ùõΩ\n",
        "Œ≤: Decay factor (e.g., 0.9)\n",
        "ùúñ\n",
        "œµ: Small constant to prevent division by zero (e.g.,\n",
        "1\n",
        "0\n",
        "‚àí\n",
        "8\n",
        "10\n",
        "‚àí8\n",
        " )\n",
        "Example:"
      ],
      "metadata": {
        "id": "YDT3yvRsCi7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = 0\n",
        "beta = 0.9\n",
        "epsilon = 1e-8\n",
        "for epoch in range(num_epochs):\n",
        "    gradient = compute_gradient(loss_function, weights)\n",
        "    s = beta * s + (1 - beta) * gradient ** 2\n",
        "    weights = weights - learning_rate * gradient / (s ** 0.5 + epsilon)\n"
      ],
      "metadata": {
        "id": "WfzOQ6oWCnB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pros:\n",
        "\n",
        "Effective for non-stationary objectives, commonly used in RNNs.\n",
        "Cons:\n",
        "\n",
        "Requires tuning\n",
        "ùõΩ\n",
        "Œ≤ and learning rate.\n",
        "5. Adam (Adaptive Moment Estimation)\n",
        "Description:\n",
        "Adam combines ideas from Momentum and RMSProp by adapting the learning rate and using momentum for more efficient updates.\n",
        "\n",
        "Update Rules:\n",
        "\n",
        "ùëö\n",
        "ùë°\n",
        "=\n",
        "ùõΩ\n",
        "1\n",
        "ùëö\n",
        "ùë°\n",
        "‚àí\n",
        "1\n",
        "+\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùõΩ\n",
        "1\n",
        ")\n",
        "‚àá\n",
        "ùúÉ\n",
        "ùêΩ\n",
        "(\n",
        "ùúÉ\n",
        ")\n",
        "m\n",
        "t\n",
        "‚Äã\n",
        " =Œ≤\n",
        "1\n",
        "‚Äã\n",
        " m\n",
        "t‚àí1\n",
        "‚Äã\n",
        " +(1‚àíŒ≤\n",
        "1\n",
        "‚Äã\n",
        " )‚àá\n",
        "Œ∏\n",
        "‚Äã\n",
        " J(Œ∏)\n",
        "ùë£\n",
        "ùë°\n",
        "=\n",
        "ùõΩ\n",
        "2\n",
        "ùë£\n",
        "ùë°\n",
        "‚àí\n",
        "1\n",
        "+\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùõΩ\n",
        "2\n",
        ")\n",
        "(\n",
        "‚àá\n",
        "ùúÉ\n",
        "ùêΩ\n",
        "(\n",
        "ùúÉ\n",
        ")\n",
        ")\n",
        "2\n",
        "v\n",
        "t\n",
        "‚Äã\n",
        " =Œ≤\n",
        "2\n",
        "‚Äã\n",
        " v\n",
        "t‚àí1\n",
        "‚Äã\n",
        " +(1‚àíŒ≤\n",
        "2\n",
        "‚Äã\n",
        " )(‚àá\n",
        "Œ∏\n",
        "‚Äã\n",
        " J(Œ∏))\n",
        "2\n",
        "\n",
        "ùúÉ\n",
        "=\n",
        "ùúÉ\n",
        "‚àí\n",
        "ùúÇ\n",
        "ùëö\n",
        "ùë°\n",
        "/\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùõΩ\n",
        "1\n",
        "ùë°\n",
        ")\n",
        "ùë£\n",
        "ùë°\n",
        "/\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùõΩ\n",
        "2\n",
        "ùë°\n",
        ")\n",
        "+\n",
        "ùúñ\n",
        "Œ∏=Œ∏‚àíŒ∑\n",
        "v\n",
        "t\n",
        "‚Äã\n",
        " /(1‚àíŒ≤\n",
        "2\n",
        "t\n",
        "‚Äã\n",
        " )\n",
        "‚Äã\n",
        " +œµ\n",
        "m\n",
        "t\n",
        "‚Äã\n",
        " /(1‚àíŒ≤\n",
        "1\n",
        "t\n",
        "‚Äã\n",
        " )\n",
        "‚Äã\n",
        "\n",
        "ùëö\n",
        "ùë°\n",
        "m\n",
        "t\n",
        "‚Äã\n",
        " : First moment estimate (mean of gradients)\n",
        "ùë£\n",
        "ùë°\n",
        "v\n",
        "t\n",
        "‚Äã\n",
        " : Second moment estimate (variance of gradients)\n",
        "ùõΩ\n",
        "1\n",
        "‚âà\n",
        "0.9\n",
        "Œ≤\n",
        "1\n",
        "‚Äã\n",
        " ‚âà0.9,\n",
        "ùõΩ\n",
        "2\n",
        "‚âà\n",
        "0.999\n",
        "Œ≤\n",
        "2\n",
        "‚Äã\n",
        " ‚âà0.999,\n",
        "ùúñ\n",
        "‚âà\n",
        "1\n",
        "0\n",
        "‚àí\n",
        "8\n",
        "œµ‚âà10\n",
        "‚àí8\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "n-3Qp17DCqs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        loss = compute_loss(model, batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "id": "Pf_N9l25Cvvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pros:\n",
        "\n",
        "Adaptive learning rates, less need for tuning.\n",
        "Works well in practice for many deep learning models.\n",
        "Cons:\n",
        "\n",
        "Can be computationally expensive.\n",
        "Summary of Optimizers\n",
        "Optimizer\tKey Feature\tUse Case\n",
        "Gradient Descent\tEntire dataset per update\tSmall datasets, simple models\n",
        "SGD\tSingle or small batch updates\tLarge datasets\n",
        "Momentum\tAdds velocity to reduce oscillations\tFaster convergence in deep networks\n",
        "RMSProp\tAdaptive learning rate\tRNNs, non-stationary data\n",
        "Adam\tCombines Momentum and RMSProp\tGeneral-purpose optimizer for deep learning\n",
        "Choosing the right optimizer depends on the dataset, model architecture, and task complexity."
      ],
      "metadata": {
        "id": "dSllNpoECyQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q17 What is sklearn.linear_model ?"
      ],
      "metadata": {
        "id": "-w9zSwWoC2_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans sklearn.linear_model in Scikit-Learn\n",
        "sklearn.linear_model is a module in the Scikit-Learn library that provides a collection of linear models for regression and classification tasks. These models assume a linear relationship between the input features and the target variable and are widely used due to their simplicity and efficiency.\n",
        "\n",
        "Key Features of sklearn.linear_model\n",
        "Diverse Models: Offers several linear models for different use cases.\n",
        "Efficiency: Fast to train, especially for large datasets.\n",
        "Regularization: Many models support techniques like L1 (Lasso), L2 (Ridge), and Elastic Net regularization to prevent overfitting.\n",
        "Easy Integration: Works seamlessly with other Scikit-Learn tools for preprocessing, pipelines, and evaluation.\n",
        "Common Models in sklearn.linear_model\n",
        "Here's an overview of the most commonly used models in sklearn.linear_model:\n",
        "\n",
        "1. Linear Regression\n",
        "Description: Fits a linear relationship between the input features and target variable using the least squares method.\n",
        "Use Case: Predicting continuous values."
      ],
      "metadata": {
        "id": "QFa5pl9eC8FO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "X = np.array([[1], [2], [3], [4]])\n",
        "y = np.array([2, 4, 6, 8])\n",
        "\n",
        "# Train model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict\n",
        "print(model.predict([[5]]))  # Output: [10.]\n"
      ],
      "metadata": {
        "id": "nVC8Mex-C-QS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Logistic Regression\n",
        "Description: A linear model for binary or multiclass classification tasks.\n",
        "Use Case: Classifying email as spam or not spam."
      ],
      "metadata": {
        "id": "Uu3UjsMXDBIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Example data\n",
        "X = [[0.5], [1.5], [3.5]]\n",
        "y = [0, 0, 1]\n",
        "\n",
        "# Train model\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict\n",
        "print(model.predict([[2.0]]))  # Output: [0]\n"
      ],
      "metadata": {
        "id": "7rTwEu4TDC-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Ridge Regression\n",
        "Description: Linear regression with L2 regularization to prevent overfitting.\n",
        "Use Case: When there are many correlated features."
      ],
      "metadata": {
        "id": "y5iTAiCkDFr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Train model with L2 regularization\n",
        "model = Ridge(alpha=1.0)\n",
        "model.fit(X, y)\n"
      ],
      "metadata": {
        "id": "Z39RON2LDHme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Lasso Regression\n",
        "Description: Linear regression with L1 regularization, which can shrink some coefficients to zero (feature selection).\n",
        "Use Case: Reducing the number of features in the model."
      ],
      "metadata": {
        "id": "VsvGA0VRDMYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "# Train model with L1 regularization\n",
        "model = Lasso(alpha=0.1)\n",
        "model.fit(X, y)\n"
      ],
      "metadata": {
        "id": "X_ll1KdlDOiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Elastic Net\n",
        "Description: Combines both L1 and L2 regularization.\n",
        "Use Case: When you need a balance between Ridge and Lasso."
      ],
      "metadata": {
        "id": "U6EtpcmYDRLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "# Train model with both L1 and L2 regularization\n",
        "model = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
        "model.fit(X, y)\n"
      ],
      "metadata": {
        "id": "vjK0Rrn1DS2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. SGDClassifier / SGDRegressor\n",
        "Description: Stochastic Gradient Descent (SGD) for large-scale linear classification or regression tasks.\n",
        "Use Case: Large datasets where batch gradient descent is inefficient.\n",
        "python\n",
        "Copy code\n"
      ],
      "metadata": {
        "id": "kpY57lh1DU3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# Train classifier with SGD\n",
        "model = SGDClassifier(max_iter=1000, learning_rate='constant', eta0=0.01)\n",
        "model.fit(X, y)\n"
      ],
      "metadata": {
        "id": "k4wu3vedDWbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of Models in sklearn.linear_model\n",
        "Model\tDescription\tUse Case\n",
        "LinearRegression\tOrdinary least squares regression\tPredicting continuous targets\n",
        "LogisticRegression\tLogistic regression for classification\tBinary/multiclass classification\n",
        "Ridge\tL2-regularized regression\tPreventing overfitting (correlated features)\n",
        "Lasso\tL1-regularized regression (feature selection)\tSparse models (reducing feature count)\n",
        "ElasticNet\tCombination of L1 and L2 regularization\tBalancing feature selection and stability\n",
        "SGDClassifier\tStochastic Gradient Descent for classification\tLarge-scale classification\n",
        "SGDRegressor\tStochastic Gradient Descent for regression\tLarge-scale regression\n",
        "Key Parameters in sklearn.linear_model\n",
        "fit_intercept: Whether to calculate the intercept (\n",
        "ùëè\n",
        "b).\n",
        "Example: fit_intercept=True\n",
        "normalize: Whether to normalize the input data (deprecated in recent versions).\n",
        "alpha: Regularization strength (for Ridge, Lasso, ElasticNet).\n",
        "solver: Optimization algorithm (for Logistic Regression).\n",
        "Example: solver='liblinear' for smaller datasets.\n",
        "Conclusion\n",
        "The sklearn.linear_model module is a versatile set of tools for solving linear regression and classification problems. Its ease of use, efficiency, and regularization options make it a fundamental part of any data scientist's toolkit."
      ],
      "metadata": {
        "id": "dAHDNjxODaev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q18 What does model.fit() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "6KZiykjWDfB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans What Does model.fit() Do?\n",
        "The fit() method in machine learning libraries like Scikit-Learn is used to train a model on a given dataset. When you call model.fit(), the model learns the relationship between the input features (X) and the target variable (y). It adjusts the internal parameters (like weights and biases) to minimize the error according to the specified loss function or objective.\n",
        "\n",
        "General Syntax"
      ],
      "metadata": {
        "id": "tPH6HM3IDiwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y)\n"
      ],
      "metadata": {
        "id": "7sq8PnjADk73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "X: The input features (independent variables). Usually a 2D array, DataFrame, or matrix with shape (n_samples, n_features).\n",
        "y: The target variable (dependent variable). Usually a 1D array with shape (n_samples,) or (n_samples, n_targets) for multi-output tasks.\n",
        "What Happens During fit()?\n",
        "Initialization: The model initializes its parameters (e.g., weights).\n",
        "Optimization: The model iterates through the data and adjusts the parameters based on the gradients of the loss function (e.g., using optimizers like gradient descent).\n",
        "Convergence: Training continues until a stopping condition is met (e.g., reaching a maximum number of iterations or minimal error change).\n",
        "After calling fit(), the model is ready to make predictions using predict().\n",
        "\n",
        "Required Arguments for fit()\n",
        "X (Features):\n",
        "\n",
        "Type: array-like, list, pandas.DataFrame, or numpy.ndarray.\n",
        "Shape: (n_samples, n_features)\n",
        "Each row represents one sample, and each column represents one feature.\n",
        "y (Target/Labels):\n",
        "\n",
        "Type: array-like or list\n",
        "Shape: (n_samples,) for single-output tasks or (n_samples, n_targets) for multi-output tasks.\n",
        "Contains the labels or target values corresponding to each sample.\n",
        "Example for Linear Regression in Scikit-Learn"
      ],
      "metadata": {
        "id": "iwA2lmLPDoPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Sample data (X: Features, y: Target)\n",
        "X = np.array([[1], [2], [3], [4]])\n",
        "y = np.array([2, 4, 6, 8])\n",
        "\n",
        "# Initialize and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "print(model.predict([[5]]))  # Output: [10.]\n"
      ],
      "metadata": {
        "id": "6IDmBftwDqCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional Parameters in fit()\n",
        "Some models have additional optional arguments for fit(). Examples include:\n",
        "\n",
        "sample_weight: An array of weights to give different importance to different samples during training.\n",
        "\n",
        "Example: model.fit(X, y, sample_weight=[1, 0.5, 2, 1])\n",
        "epochs: Number of iterations through the dataset (for neural networks in libraries like Keras).\n",
        "\n",
        "Example: model.fit(X, y, epochs=50)\n",
        "verbose: Controls the output during training (for libraries like Keras).\n",
        "\n",
        "Example: model.fit(X, y, verbose=1)\n",
        "Example in Keras (Neural Networks)\n",
        "python\n",
        "Copy code\n"
      ],
      "metadata": {
        "id": "VsUy0jhDDuCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1], [2], [3], [4]])\n",
        "y = np.array([2, 4, 6, 8])\n",
        "\n",
        "# Define a simple neural network\n",
        "model = Sequential([Dense(1, input_shape=(1,))])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
        "\n",
        "# Train the model with optional parameters\n",
        "model.fit(X, y, epochs=100, verbose=1)\n"
      ],
      "metadata": {
        "id": "K0dkBdsKDv_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "fit() trains the model using the provided features (X) and target (y).\n",
        "Required Arguments:\n",
        "X: Input features with shape (n_samples, n_features).\n",
        "y: Target values with shape (n_samples,).\n",
        "Optional Arguments: Vary by model and library (e.g., sample_weight, epochs, verbose)."
      ],
      "metadata": {
        "id": "EuXnumOUDypb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19 What does model.predict() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "2QNQBZFUD11e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans What Does model.predict() Do?\n",
        "The predict() method in machine learning libraries like Scikit-Learn and Keras is used to generate predictions using a trained model. Once a model has been trained with fit(), calling predict() applies the model's learned parameters (weights and biases) to new input data to produce output predictions.\n",
        "\n",
        "General Syntax"
      ],
      "metadata": {
        "id": "pN2YjUnGD7eh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(X_new)\n"
      ],
      "metadata": {
        "id": "la0x5tf5D9n1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "X_new: The new input features for which predictions are needed.\n",
        "predictions: The output predictions generated by the model.\n",
        "What Happens During predict()?\n",
        "Forward Pass: The input data (X_new) is passed through the trained model.\n",
        "Computation: The model uses its learned parameters (weights and biases) to calculate predictions based on the input data.\n",
        "Output: The predicted values are returned.\n",
        "Required Arguments for predict()\n",
        "X_new:\n",
        "Type: array-like, list, pandas.DataFrame, or numpy.ndarray.\n",
        "Shape: (n_samples, n_features)\n",
        "Each row represents one sample, and each column represents one feature.\n",
        "Example with Scikit-Learn\n",
        "Linear Regression Example\n",
        "python\n",
        "Copy code\n"
      ],
      "metadata": {
        "id": "qoZsZ_FFECVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Sample data for training\n",
        "X_train = np.array([[1], [2], [3], [4]])\n",
        "y_train = np.array([2, 4, 6, 8])\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# New data for prediction\n",
        "X_new = np.array([[5], [6]])\n",
        "\n",
        "# Generate predictions\n",
        "predictions = model.predict(X_new)\n",
        "print(predictions)  # Output: [10. 12.]\n"
      ],
      "metadata": {
        "id": "ObQgRmPJED2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example with Classification (Logistic Regression)"
      ],
      "metadata": {
        "id": "G4WEyW5XEGLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Sample training data\n",
        "X_train = [[0], [1], [2], [3]]\n",
        "y_train = [0, 0, 1, 1]\n",
        "\n",
        "# Train the classifier\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# New data for prediction\n",
        "X_new = [[1.5], [2.5]]\n",
        "\n",
        "# Generate predictions\n",
        "predictions = model.predict(X_new)\n",
        "print(predictions)  # Output: [0 1]\n"
      ],
      "metadata": {
        "id": "xSjbQCzGEHqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output of predict()\n",
        "For Regression:\n",
        "Returns continuous values (e.g., [10.5, 12.0]).\n",
        "\n",
        "For Classification:\n",
        "Returns class labels (e.g., [0, 1] for binary classification).\n",
        "\n",
        "Optional Parameters for predict()\n",
        "Some libraries offer additional options for predict(). For example:\n",
        "\n",
        "return_std (for Gaussian Process Regressors):\n",
        "Returns standard deviations along with predictions.\n",
        "Example: model.predict(X_new, return_std=True)\n",
        "Difference Between predict() and predict_proba()\n",
        "In classification models:\n",
        "\n",
        "predict(): Returns the class labels (e.g., [0, 1, 0]).\n",
        "predict_proba(): Returns the probabilities of each class (e.g., [[0.8, 0.2], [0.3, 0.7]]).\n",
        "Example:"
      ],
      "metadata": {
        "id": "-CCkuiQMELt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(X_new)        # Output: [0 1]\n",
        "probabilities = model.predict_proba(X_new) # Output: [[0.8, 0.2], [0.3, 0.7]]\n"
      ],
      "metadata": {
        "id": "wimbjD4QEPbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "predict() generates predictions based on new input data (X_new).\n",
        "Required Argument:\n",
        "X_new: New data with shape (n_samples, n_features).\n",
        "Output:\n",
        "Regression: Continuous values.\n",
        "Classification: Class labels.\n",
        "Use predict_proba() if you need class probabilities instead of labels.\n",
        "The predict() method is a crucial part of using a trained model to make real-world predictions."
      ],
      "metadata": {
        "id": "92lht5XpESVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20 What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "3VnGUrvKEWo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans Continuous and Categorical Variables in Data Analysis\n",
        "In data analysis and statistics, variables are typically classified as continuous or categorical. These types help determine the methods used for analysis, visualization, and modeling.\n",
        "\n",
        "1. Continuous Variables\n",
        "Definition\n",
        "A continuous variable (also called a quantitative or numerical variable) can take on an infinite range of values within a specified range. The values are typically measurable and can include fractions or decimals.\n",
        "\n",
        "Examples\n",
        "Height: 165.2 cm, 170.8 cm\n",
        "Temperature: 37.5¬∞C, 21.3¬∞C\n",
        "Weight: 68.5 kg, 72.9 kg\n",
        "Time: 2.45 seconds, 3.78 hours\n",
        "Key Characteristics\n",
        "Measured on a scale (interval or ratio scale).\n",
        "Can be subdivided into finer units (e.g., 1.23, 1.234, 1.2345).\n",
        "Supports mathematical operations (addition, subtraction, etc.).\n",
        "Types of Continuous Variables\n",
        "Interval Variable:\n",
        "\n",
        "Measured on a scale where differences are meaningful, but there's no true zero.\n",
        "Example: Temperature in Celsius (0¬∞C does not mean \"no temperature\").\n",
        "Ratio Variable:\n",
        "\n",
        "Measured on a scale with a true zero point, meaning zero indicates the absence of the quantity.\n",
        "Example: Weight (0 kg means no weight).\n",
        "2. Categorical Variables\n",
        "Definition\n",
        "A categorical variable (also called a qualitative variable) represents data that can be grouped into categories or labels. These categories have no inherent numerical value or order unless specified (as in ordinal variables).\n",
        "\n",
        "Examples\n",
        "Gender: Male, Female, Non-binary\n",
        "Color: Red, Blue, Green\n",
        "City: New York, Paris, Tokyo\n",
        "Customer Satisfaction: Satisfied, Neutral, Unsatisfied\n",
        "Key Characteristics\n",
        "Describes a quality or characteristic.\n",
        "Categories are often represented by labels or names.\n",
        "Cannot perform typical arithmetic operations.\n",
        "Types of Categorical Variables\n",
        "Nominal Variable:\n",
        "\n",
        "Categories have no natural order or ranking.\n",
        "Example: Color (Red, Green, Blue).\n",
        "Ordinal Variable:\n",
        "\n",
        "Categories have a meaningful order or ranking, but the intervals between them may not be uniform.\n",
        "Example: Satisfaction levels (Satisfied, Neutral, Unsatisfied).\n",
        "Key Differences Between Continuous and Categorical Variables\n",
        "Feature\tContinuous Variables\tCategorical Variables\n",
        "Nature\tNumerical, measurable\tDescriptive, qualitative\n",
        "Possible Values\tInfinite range (e.g., 1.25, 3.75)\tFixed categories (e.g., Red, Green, Blue)\n",
        "Operations\tSupports arithmetic operations\tNo arithmetic operations\n",
        "Examples\tHeight, Weight, Temperature\tGender, Color, City\n",
        "Types\tInterval, Ratio\tNominal, Ordinal\n",
        "Examples in Python\n",
        "Continuous Variable Example"
      ],
      "metadata": {
        "id": "SubfcQP1Edbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example DataFrame with continuous variables\n",
        "data = pd.DataFrame({\n",
        "    'Height_cm': [165.2, 170.8, 180.1],\n",
        "    'Weight_kg': [68.5, 72.9, 85.3]\n",
        "})\n",
        "\n",
        "print(data)\n"
      ],
      "metadata": {
        "id": "jYwFmsuSEf-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output:"
      ],
      "metadata": {
        "id": "WB-UOidcEiGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "   Height_cm  Weight_kg\n",
        "0      165.2       68.5\n",
        "1      170.8       72.9\n",
        "2      180.1       85.3\n"
      ],
      "metadata": {
        "id": "krDYESJTEj8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Categorical Variable Example"
      ],
      "metadata": {
        "id": "BAyy49faElzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example DataFrame with categorical variables\n",
        "data = pd.DataFrame({\n",
        "    'Gender': ['Male', 'Female', 'Non-binary'],\n",
        "    'Color': ['Red', 'Blue', 'Green']\n",
        "})\n",
        "\n",
        "print(data)\n"
      ],
      "metadata": {
        "id": "wmP86A8KEnWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output:"
      ],
      "metadata": {
        "id": "vySH23IwErCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "       Gender  Color\n",
        "0        Male    Red\n",
        "1      Female   Blue\n",
        "2  Non-binary  Green\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "8rzt7BBsEsmh",
        "outputId": "5dae3503-cb77-48e6-9466-326ca196a1d9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-28-16025f8c49b9>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-28-16025f8c49b9>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Gender  Color\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why It Matters in Data Analysis\n",
        "Choosing Models:\n",
        "\n",
        "Regression models typically require continuous variables.\n",
        "Classification models often use categorical target variables.\n",
        "Preprocessing:\n",
        "\n",
        "Continuous data might require scaling (e.g., Standardization).\n",
        "Categorical data might need encoding (e.g., One-Hot Encoding).\n",
        "Visualization:\n",
        "\n",
        "Continuous data: Histograms, Scatter Plots.\n",
        "Categorical data: Bar Charts, Pie Charts.\n",
        "Understanding whether a variable is continuous or categorical helps in selecting the right analytical approach and preprocessing techniques."
      ],
      "metadata": {
        "id": "cWbl24faEuni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q21 What is feature scaling? How does it help in Machine Learning?"
      ],
      "metadata": {
        "id": "85KlxJvVE2Xd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans What is Feature Scaling?\n",
        "Feature scaling is a preprocessing technique used in machine learning to normalize or standardize the range of independent variables (features). This ensures that all features contribute equally to the model's learning process. Feature scaling adjusts the values of features to a common scale, typically without distorting their relative distribution or relationships.\n",
        "\n",
        "Why Feature Scaling is Important in Machine Learning\n",
        "Algorithms are Sensitive to Feature Magnitude:\n",
        "Some machine learning models (e.g., gradient-based methods) are influenced by the scale of the features. If features have vastly different scales, the model may favor features with larger numerical values, leading to biased or suboptimal results.\n",
        "\n",
        "Improves Convergence Speed:\n",
        "For gradient-based optimization algorithms like Stochastic Gradient Descent (SGD), scaling can help the model converge faster and avoid local minima.\n",
        "\n",
        "Ensures Fair Weight Assignments:\n",
        "In models like Linear Regression, Logistic Regression, and Neural Networks, features with larger scales can dominate the learning process. Scaling ensures all features contribute proportionately.\n",
        "\n",
        "Distance-Based Algorithms:\n",
        "Algorithms that rely on distances between data points (e.g., K-Nearest Neighbors (KNN), Support Vector Machines (SVM), and K-Means Clustering) require features to be on similar scales for accurate distance calculations.\n",
        "\n",
        "Improves Performance of PCA:\n",
        "In Principal Component Analysis (PCA), feature scaling ensures that features with larger scales don‚Äôt dominate the principal components.\n",
        "\n",
        "Common Methods for Feature Scaling\n",
        "1. Min-Max Scaling (Normalization)\n",
        "Formula:\n",
        "\n",
        "ùëã\n",
        "scaled\n",
        "=\n",
        "ùëã\n",
        "‚àí\n",
        "ùëã\n",
        "min\n",
        "ùëã\n",
        "max\n",
        "‚àí\n",
        "ùëã\n",
        "min\n",
        "X\n",
        "scaled\n",
        "‚Äã\n",
        " =\n",
        "X\n",
        "max\n",
        "‚Äã\n",
        " ‚àíX\n",
        "min\n",
        "‚Äã\n",
        "\n",
        "X‚àíX\n",
        "min\n",
        "‚Äã\n",
        "\n",
        "‚Äã\n",
        "\n",
        "Range: Scales features to a range of [0, 1] (or a custom range).\n",
        "\n",
        "Use Case: When the distribution is not Gaussian, or when you need values bounded between 0 and 1.\n",
        "\n",
        "Example in Python:"
      ],
      "metadata": {
        "id": "tipu98f6E_3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[10], [20], [30], [40]])\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "print(scaled_data)\n"
      ],
      "metadata": {
        "id": "rJaDcHXDFCYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output:"
      ],
      "metadata": {
        "id": "pGZZ3YttFFPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[[0.  ]\n",
        " [0.33]\n",
        " [0.67]\n",
        " [1.  ]]\n"
      ],
      "metadata": {
        "id": "GldQpDTTFIqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Standardization (Z-Score Scaling)\n",
        "Formula:\n",
        "\n",
        "ùëã\n",
        "scaled\n",
        "=\n",
        "ùëã\n",
        "‚àí\n",
        "ùúá\n",
        "ùúé\n",
        "X\n",
        "scaled\n",
        "‚Äã\n",
        " =\n",
        "œÉ\n",
        "X‚àíŒº\n",
        "‚Äã\n",
        "\n",
        "where\n",
        "ùúá\n",
        "Œº is the mean and\n",
        "ùúé\n",
        "œÉ is the standard deviation.\n",
        "\n",
        "Range: Scales data to have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "Use Case: When the data follows a Gaussian (normal) distribution or when algorithms assume standardized data (e.g., SVM, Logistic Regression).\n",
        "\n",
        "Example in Python:"
      ],
      "metadata": {
        "id": "VCwoX0L7FLHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[10], [20], [30], [40]])\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "print(scaled_data)\n"
      ],
      "metadata": {
        "id": "7eMRdHosFN6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output:"
      ],
      "metadata": {
        "id": "m9PFLStxFPst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[[-1.34]\n",
        " [-0.45]\n",
        " [ 0.45]\n",
        " [ 1.34]]\n"
      ],
      "metadata": {
        "id": "sPvTBedGFR5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Max Absolute Scaling\n",
        "Formula:\n",
        "\n",
        "ùëã\n",
        "scaled\n",
        "=\n",
        "ùëã\n",
        "max\n",
        "‚Å°\n",
        "(\n",
        "‚à£\n",
        "ùëã\n",
        "‚à£\n",
        ")\n",
        "X\n",
        "scaled\n",
        "‚Äã\n",
        " =\n",
        "max(‚à£X‚à£)\n",
        "X\n",
        "‚Äã\n",
        "\n",
        "Range: Scales data to the range [-1, 1] based on the maximum absolute value.\n",
        "\n",
        "Use Case: When the data is centered around zero.\n",
        "\n",
        "Example in Python:"
      ],
      "metadata": {
        "id": "rDvD3HS0FUXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "\n",
        "data = np.array([[1, -2], [3, -6], [4, -8]])\n",
        "scaler = MaxAbsScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "print(scaled_data)\n"
      ],
      "metadata": {
        "id": "cQY_26IMFd4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output:"
      ],
      "metadata": {
        "id": "oV8WirOQFgKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[[ 0.25 -0.25]\n",
        " [ 0.75 -0.75]\n",
        " [ 1.   -1.  ]]\n"
      ],
      "metadata": {
        "id": "Q6k0EOp3FicR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Robust Scaling\n",
        "Formula:\n",
        "\n",
        "ùëã\n",
        "scaled\n",
        "=\n",
        "ùëã\n",
        "‚àí\n",
        "median\n",
        "IQR\n",
        "X\n",
        "scaled\n",
        "‚Äã\n",
        " =\n",
        "IQR\n",
        "X‚àímedian\n",
        "‚Äã\n",
        "\n",
        "where IQR (Interquartile Range) = Q3 - Q1 (75th percentile - 25th percentile).\n",
        "\n",
        "Range: Centers data around the median and scales based on the IQR.\n",
        "\n",
        "Use Case: When the data contains outliers.\n",
        "\n",
        "Example in Python:"
      ],
      "metadata": {
        "id": "g0FSjH3cFmxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "data = np.array([[1], [2], [100]])\n",
        "scaler = RobustScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "print(scaled_data)\n"
      ],
      "metadata": {
        "id": "dX41IQPvFpoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output:"
      ],
      "metadata": {
        "id": "cyeV_ogAFuTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[[-0.5]\n",
        " [ 0. ]\n",
        " [ 9.5]]\n"
      ],
      "metadata": {
        "id": "PQ_hJ8a5FwDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When to Use Each Scaling Method\n",
        "Method\tBest Use Case\n",
        "Min-Max Scaling\tWhen features need to be in a specific range (e.g., [0, 1]). Ideal for neural networks.\n",
        "Standardization\tWhen data is normally distributed or when using algorithms that assume standard-normal distributions (e.g., SVM, Logistic Regression).\n",
        "Max Abs Scaling\tFor sparse data or data centered around zero.\n",
        "Robust Scaling\tWhen data contains significant outliers.\n",
        "Summary\n",
        "Feature scaling transforms the range or distribution of features.\n",
        "It helps models converge faster, improves accuracy, and ensures fair feature contribution.\n",
        "Common methods: Min-Max Scaling, Standardization, Max Absolute Scaling, and Robust Scaling.\n",
        "Proper scaling depends on the nature of your data and the model you're using."
      ],
      "metadata": {
        "id": "PvW5jq9XFzVx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q22 How do we perform scaling in Python?"
      ],
      "metadata": {
        "id": "icpFPML5F5O5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans Scaling in Python typically refers to the process of adjusting the values of numerical features in a dataset to fit a specific range or distribution, which is often essential for machine learning algorithms to perform optimally. Here are the common methods to perform scaling:\n",
        "\n",
        "1. Min-Max Scaling (Normalization)\n",
        "This method scales the data to a specific range, usually between 0 and 1. It is sensitive to outliers"
      ],
      "metadata": {
        "id": "4U2m8_NIF-Z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Create an instance of MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "scaled_data = scaler.fit_transform(data)  # data is your input dataset\n"
      ],
      "metadata": {
        "id": "_AyuyHKbGAxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Standardization (Z-Score Scaling)\n",
        "Standardization scales the data to have a mean of 0 and a standard deviation of 1. It is less sensitive to outliers compared to Min-Max scaling."
      ],
      "metadata": {
        "id": "bHwqDlmGGExi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create an instance of StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "scaled_data = scaler.fit_transform(data)  # data is your input dataset\n"
      ],
      "metadata": {
        "id": "yNeSih2XGGg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Robust Scaling\n",
        "This method uses the median and interquartile range, making it more robust to outliers compared to Min-Max scaling and Standardization."
      ],
      "metadata": {
        "id": "nOylZKiwGO9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Create an instance of RobustScaler\n",
        "scaler = RobustScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "scaled_data = scaler.fit_transform(data)  # data is your input dataset\n"
      ],
      "metadata": {
        "id": "X0v1zSAiGQ01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Max Abs Scaling\n",
        "This method scales each feature by its maximum absolute value, keeping values between -1 and 1. It is useful when data is sparse and you don't want to shift values.\n",
        "\n",
        "python\n",
        "Copy code\n"
      ],
      "metadata": {
        "id": "nemiNIObGSoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "\n",
        "# Create an instance of MaxAbsScaler\n",
        "scaler = MaxAbsScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "scaled_data = scaler.fit_transform(data)  # data is your input dataset\n"
      ],
      "metadata": {
        "id": "ki6nopatGUON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example:\n",
        "Here‚Äôs an example of using these methods:"
      ],
      "metadata": {
        "id": "01ATFoAOGWAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "# Sample data\n",
        "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "\n",
        "# Min-Max scaling\n",
        "min_max_scaler = MinMaxScaler()\n",
        "data_min_max = min_max_scaler.fit_transform(data)\n",
        "print(\"Min-Max Scaling:\\n\", data_min_max)\n",
        "\n",
        "# Standardization\n",
        "standard_scaler = StandardScaler()\n",
        "data_standardized = standard_scaler.fit_transform(data)\n",
        "print(\"Standardized Data:\\n\", data_standardized)\n"
      ],
      "metadata": {
        "id": "LLBEQgtWGXqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can choose the appropriate scaling technique based on the nature of your data and the machine learning model you're using"
      ],
      "metadata": {
        "id": "m8AGxEM9Gaef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q23 What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "UGASaMmuGemn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans sklearn.preprocessing is a module in the Scikit-learn library that provides various utilities for preprocessing data. These utilities help transform or scale features in a dataset before feeding them into machine learning models, ensuring better performance and faster convergence.\n",
        "\n",
        "The module includes tools for scaling, normalizing, encoding, and imputing missing values. Here are some key classes and functions from sklearn.preprocessing:\n",
        "\n",
        "1. Scaling and Normalization\n",
        "MinMaxScaler: Scales the data to a specified range, usually [0, 1]"
      ],
      "metadata": {
        "id": "L56f0B2VGniS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n"
      ],
      "metadata": {
        "id": "Teo_RCA2GqAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "StandardScaler: Scales data so that it has a mean of 0 and a standard deviation of 1 (Z-score scaling)."
      ],
      "metadata": {
        "id": "0fwS49UfGtRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n"
      ],
      "metadata": {
        "id": "jIapc-X9GvOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RobustScaler: Scales data using the median and interquartile range (IQR), making it less sensitive to outliers"
      ],
      "metadata": {
        "id": "KwV-Ya2yGzGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "scaler = RobustScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n"
      ],
      "metadata": {
        "id": "wlKZesRGG0rO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MaxAbsScaler: Scales data by its maximum absolute value, keeping values between -1 and 1."
      ],
      "metadata": {
        "id": "dX-3lqQIG4Hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "scaler = MaxAbsScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n"
      ],
      "metadata": {
        "id": "0Jbw5hf9G50c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding Categorical Features\n",
        "LabelEncoder: Converts categorical labels (strings or non-numeric labels) into numeric labels"
      ],
      "metadata": {
        "id": "GM3R-JV5G8XK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "data_encoded = encoder.fit_transform(categorical_data)\n"
      ],
      "metadata": {
        "id": "tuIs9pBOG-O_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OneHotEncoder: Converts categorical variables into one-hot encoded vectors (binary columns)"
      ],
      "metadata": {
        "id": "NRQmeexwHAot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder()\n",
        "data_encoded = encoder.fit_transform(categorical_data)\n"
      ],
      "metadata": {
        "id": "ynZCrqSsHCoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imputation (Handling Missing Data)\n",
        "SimpleImputer: Fills in missing values with a specified strategy (mean, median, most frequent, or constant)."
      ],
      "metadata": {
        "id": "NgOWxSbQHGQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "data_imputed = imputer.fit_transform(data)\n"
      ],
      "metadata": {
        "id": "U40aisQ5HPOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Polynomial Features\n",
        "PolynomialFeatures: Generates polynomial features (i.e., powers of original features) for non-linear models."
      ],
      "metadata": {
        "id": "fsx0J9YjHSof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "data_poly = poly.fit_transform(data)\n"
      ],
      "metadata": {
        "id": "lDE_DlRkHUwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Binarization\n",
        "Binarizer: Binarizes data (converts to 0s and 1s based on a threshold"
      ],
      "metadata": {
        "id": "UYGVhxA3HfKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import Binarizer\n",
        "binarizer = Binarizer(threshold=0)\n",
        "data_binarized = binarizer.fit_transform(data)\n"
      ],
      "metadata": {
        "id": "zNfrV9taHhDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FunctionTransformer\n",
        "FunctionTransformer: Allows the application of a custom function to transform the data"
      ],
      "metadata": {
        "id": "bMV9B9TyHkzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import FunctionTransformer\n",
        "transformer = FunctionTransformer(func)\n",
        "transformed_data = transformer.fit_transform(data)\n"
      ],
      "metadata": {
        "id": "EzeXV9pZHmXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example Workflow Using sklearn.preprocessing:"
      ],
      "metadata": {
        "id": "soG_oqVRHohW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "\n",
        "# Sample data (numerical and categorical)\n",
        "numerical_data = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "categorical_data = ['cat', 'dog', 'cat']\n",
        "\n",
        "# Scale the numerical data\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(numerical_data)\n",
        "\n",
        "# Encode categorical data\n",
        "encoder = LabelEncoder()\n",
        "encoded_labels = encoder.fit_transform(categorical_data)\n",
        "\n",
        "print(\"Scaled Data:\\n\", scaled_data)\n",
        "print(\"Encoded Labels:\", encoded_labels)\n"
      ],
      "metadata": {
        "id": "noos5238Hqku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary, sklearn.preprocessing provides essential tools for preparing your data before applying machine learning algorithms, especially when the dataset includes different types of data, such as numerical and categorical features."
      ],
      "metadata": {
        "id": "im6p6cqwHtFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q24 How do we split data for model fitting (training and testing) in Python?"
      ],
      "metadata": {
        "id": "0x3cQv3xH1ol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans To split data into training and testing sets in Python, you can use the train_test_split function from the Scikit-learn library. This function allows you to randomly divide your dataset into two subsets: one for training the model and the other for testing (or validating) it.\n",
        "\n",
        "Syntax:"
      ],
      "metadata": {
        "id": "jmU0xysaH6Hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "Tx413W6EH81q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters:\n",
        "X: The feature matrix (independent variables) ‚Äî typically a 2D array or DataFrame.\n",
        "y: The target vector (dependent variable) ‚Äî typically a 1D array or Series.\n",
        "test_size: The proportion of the dataset to include in the test split. It is a float between 0 and 1. For example, test_size=0.2 means 20% of the data will be used for testing and 80% for training.\n",
        "train_size: The proportion of the dataset to include in the train split. If train_size is not provided, it is inferred from the test_size.\n",
        "random_state: An integer seed for random number generation, ensuring reproducibility of the data split.\n",
        "shuffle: Whether to shuffle the data before splitting. Default is True. If set to False, the data will be split in the original order.\n",
        "stratify: Used for ensuring that the target variable y is split evenly across the training and testing sets (especially useful for imbalanced datasets).\n",
        "Example:\n",
        "Let's assume you have a dataset with features X and labels y."
      ],
      "metadata": {
        "id": "ZsQyIGG5H_cT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Example feature matrix (X) and target vector (y)\n",
        "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])\n",
        "y = np.array([0, 1, 0, 1, 0, 1])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "print(\"Training Features:\\n\", X_train)\n",
        "print(\"Testing Features:\\n\", X_test)\n",
        "print(\"Training Labels:\\n\", y_train)\n",
        "print(\"Testing Labels:\\n\", y_test)\n"
      ],
      "metadata": {
        "id": "7FwBDqc2IBpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output:\n",
        "The function will split the data into training and testing sets, based on the test_size parameter. For example:\n",
        "\n",
        "lua\n",
        "Copy code\n"
      ],
      "metadata": {
        "id": "buRni_X_IEFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Training Features:\n",
        " [[ 7  8]\n",
        " [ 1  2]\n",
        " [ 9 10]\n",
        " [11 12]]\n",
        "Testing Features:\n",
        " [[5 6]\n",
        " [3 4]]\n",
        "Training Labels:\n",
        " [1 0 0 1]\n",
        "Testing Labels:\n",
        " [0 1]\n"
      ],
      "metadata": {
        "id": "okfkIzJmIF7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Points:\n",
        "Shuffling: By default, train_test_split shuffles the data before splitting. If you want to ensure that the split follows a certain order (e.g., for time series data), you can set shuffle=False.\n",
        "Stratified Split: If your target variable y is imbalanced (e.g., one class is much more frequent than another), you may want to ensure that both the training and testing sets have a similar distribution of the classes. This can be done by setting stratify=y.\n",
        "Example of stratified splitting:"
      ],
      "metadata": {
        "id": "59EIhxNDIINl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n"
      ],
      "metadata": {
        "id": "zUWp-gLlIJ5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This ensures the proportion of labels in y remains consistent in both the training and testing sets.\n",
        "\n",
        "Conclusion:\n",
        "train_test_split is a powerful and easy-to-use function for splitting your data into training and testing sets in machine learning tasks, enabling model evaluation and validation."
      ],
      "metadata": {
        "id": "gRA7n7gFIL-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q25 Explain data encoding?"
      ],
      "metadata": {
        "id": "D3o3YbkCISNi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans Data encoding refers to the process of converting categorical data into a numerical format that machine learning algorithms can interpret. Many machine learning algorithms, particularly those based on mathematical models (e.g., linear regression, decision trees, etc.), require numerical input, so categorical variables (such as strings or labels) need to be encoded into numbers.\n",
        "\n",
        "There are different techniques to perform encoding depending on the nature of the data and the problem you're working on. Below are some common data encoding techniques:\n",
        "\n",
        "1. Label Encoding\n",
        "Label encoding converts each unique category into a numerical label. This method is simple but may introduce an unintended ordinal relationship between categories, which could mislead some models.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "3BbVrdnHIVbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "categories = ['cat', 'dog', 'cat', 'dog', 'bird']\n",
        "encoded_labels = encoder.fit_transform(categories)\n",
        "print(encoded_labels)\n"
      ],
      "metadata": {
        "id": "NeGsTEMwIhK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output:"
      ],
      "metadata": {
        "id": "oNcCJZ6qIm1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[0 1 0 1 2]\n"
      ],
      "metadata": {
        "id": "EEXMZcjEIoSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, 'cat' is encoded as 0, 'dog' as 1, and 'bird' as 2.\n",
        "\n",
        "When to use:\n",
        "\n",
        "Label encoding is useful when the categorical variable has a natural order (e.g., 'low', 'medium', 'high') and you want to preserve this ordinal relationship.\n",
        "2. One-Hot Encoding\n",
        "One-hot encoding creates binary (0 or 1) columns for each unique category in a feature. This technique avoids introducing any ordinal relationships between the categories.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "WVbfnFGUIq4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)  # sparse=False returns a dense array\n",
        "categories = np.array(['cat', 'dog', 'cat', 'dog', 'bird']).reshape(-1, 1)\n",
        "encoded = encoder.fit_transform(categories)\n",
        "print(encoded)\n"
      ],
      "metadata": {
        "id": "xQfpOfnyI0t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output:"
      ],
      "metadata": {
        "id": "ecCI2XdII22b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[[0. 1. 0.]\n",
        " [0. 0. 1.]\n",
        " [0. 1. 0.]\n",
        " [0. 0. 1.]\n",
        " [1. 0. 0.]]\n"
      ],
      "metadata": {
        "id": "gsTlrn1dI4iW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This results in three columns, one for each unique category: 'cat', 'dog', and 'bird'. For each row, a '1' is placed in the column corresponding to the category and '0' in others.\n",
        "\n",
        "When to use:\n",
        "\n",
        "One-hot encoding is suitable when there is no ordinal relationship between the categories. It is commonly used when dealing with nominal (non-ordered) categorical variables like country names, animal species, etc.\n",
        "3. Ordinal Encoding\n",
        "Ordinal encoding is a specialized form of label encoding used when there is a clear order or ranking in the categories. For example, education levels (e.g., 'high school', 'bachelor's', 'master's', 'PhD') have a natural order.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "vAaUN1UAI7m9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "categories = [['high school'], ['bachelor'], ['master'], ['PhD']]\n",
        "encoder = OrdinalEncoder()\n",
        "encoded = encoder.fit_transform(categories)\n",
        "print(encoded)\n"
      ],
      "metadata": {
        "id": "_Mf0DJ9WI_sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output:"
      ],
      "metadata": {
        "id": "GW3ApomfJCJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[[0.]\n",
        " [1.]\n",
        " [2.]\n",
        " [3.]]\n"
      ],
      "metadata": {
        "id": "PeUNxXyAJD6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the categories are encoded based on their ordinal position.\n",
        "\n",
        "When to use:\n",
        "\n",
        "Ordinal encoding is suitable when you have a categorical feature with an inherent order or ranking, such as customer satisfaction levels or education levels.\n",
        "4. Binary Encoding\n",
        "Binary encoding is a more compact method that is useful when dealing with a high cardinality (large number of unique categories). It works by first converting each category into an integer and then converting that integer to a binary representation.\n",
        "\n",
        "Example (using the category_encoders library)"
      ],
      "metadata": {
        "id": "zyZkuWQsJGfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import category_encoders as ce\n",
        "categories = ['cat', 'dog', 'bird']\n",
        "encoder = ce.BinaryEncoder()\n",
        "encoded = encoder.fit_transform(categories)\n",
        "print(encoded)\n"
      ],
      "metadata": {
        "id": "_ijsX723JIGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When to use:\n",
        "\n",
        "Binary encoding is useful when you have a large number of categories and want to reduce the dimensionality compared to one-hot encoding.\n",
        "5. Frequency Encoding\n",
        "Frequency encoding involves encoding categories based on their frequency in the dataset. This method replaces each category with the number of times it appears in the dataset.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "SamlAH4LJKPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = ['cat', 'dog', 'cat', 'dog', 'bird']\n",
        "frequency = pd.Series(data).value_counts()\n",
        "encoded = [frequency[val] for val in data]\n",
        "print(encoded)\n"
      ],
      "metadata": {
        "id": "VviJued8JL71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output:"
      ],
      "metadata": {
        "id": "eaG80QIpJOUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[2, 2, 2, 2, 1]\n"
      ],
      "metadata": {
        "id": "k6L5UL4SJPiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, 'cat' and 'dog' appear 2 times each, while 'bird' appears 1 time.\n",
        "\n",
        "When to use:\n",
        "\n",
        "Frequency encoding can be helpful when there is no intrinsic ordinal relationship, and the frequency of categories can help the model make decisions.\n",
        "6. Target Encoding\n",
        "Target encoding, also known as mean encoding, involves replacing each category with the mean of the target variable for that category. This method can be effective for high-cardinality categorical variables, but care should be taken to avoid overfitting.\n",
        "\n",
        "Example (using category_encoders)"
      ],
      "metadata": {
        "id": "2w5shxSSJSop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import category_encoders as ce\n",
        "data = pd.DataFrame({'Category': ['cat', 'dog', 'cat', 'dog', 'bird'],\n",
        "                     'Target': [1, 0, 0, 1, 0]})\n",
        "encoder = ce.TargetEncoder(cols=['Category'])\n",
        "encoded = encoder.fit_transform(data['Category'], data['Target'])\n",
        "print(encoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "fdgmbMUBJUWP",
        "outputId": "793b3eb1-6a08-461e-e4ea-c52cf08eaf35"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'category_encoders'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-270a6b43b09a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcategory_encoders\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m data = pd.DataFrame({'Category': ['cat', 'dog', 'cat', 'dog', 'bird'],\n\u001b[1;32m      3\u001b[0m                      'Target': [1, 0, 0, 1, 0]})\n\u001b[1;32m      4\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTargetEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'category_encoders'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When to use:\n",
        "\n",
        "Target encoding is used when there is a relationship between the categorical feature and the target variable. It‚Äôs useful for improving performance, especially for high-cardinality features.\n",
        "Summary of Encoding Methods:\n",
        "Encoding Type\tDescription\tWhen to Use\n",
        "Label Encoding\tAssigns a unique integer to each category\tOrdinal categories (natural order)\n",
        "One-Hot Encoding\tCreates a binary column for each category\tNominal categories (no order)\n",
        "Ordinal Encoding\tAssigns integers based on the order of categories\tCategories with a clear ranking or order\n",
        "Binary Encoding\tConverts categories into binary digits\tHigh cardinality categories (many unique values)\n",
        "Frequency Encoding\tEncodes categories by their frequency count\tWhen category frequency is meaningful\n",
        "Target Encoding\tEncodes categories by the mean of the target variable\tWhen the categorical feature is correlated with the target\n",
        "Conclusion:\n",
        "Choosing the right encoding method depends on the type of data and the nature of the problem. One-hot encoding is typically the go-to choice for categorical features without any inherent order, while label encoding is used for ordinal categories. Methods like binary encoding and target encoding are useful in specific cases, especially for handling high-cardinality features or leveraging relationships with the target variable."
      ],
      "metadata": {
        "id": "wj5KZP14JXaE"
      }
    }
  ]
}